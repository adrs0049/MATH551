{"version":"1","records":[{"hierarchy":{"lvl1":"Exercises"},"type":"lvl1","url":"/exercises","position":0},{"hierarchy":{"lvl1":"Exercises"},"content":"Practice applying Taylor’s theorem and implementing finite difference methods. These exercises build both theoretical understanding and computational skills.","type":"content","url":"/exercises","position":1},{"hierarchy":{"lvl1":"Exercises","lvl2":"Self-Assessment Questions"},"type":"lvl2","url":"/exercises#self-assessment-questions","position":2},{"hierarchy":{"lvl1":"Exercises","lvl2":"Self-Assessment Questions"},"content":"Test your understanding with these conceptual questions:\n\nTaylor’s Theorem: Can you write down Taylor’s theorem with the Lagrange form of the remainder? What conditions on f are required?\n\nError Analysis: Given a finite difference formula, can you derive its error using Taylor expansions?\n\nOrder of Accuracy: What does it mean for a method to be “first-order” or “second-order” accurate? How do you verify this computationally?\n\nImplementation: Can you implement forward and central difference approximations in Python?\n\nLog-Log Plots: Why do we use log-log plots to verify convergence rates? What slope should you expect for first-order and second-order methods?\n\nRound-off Error: Why does the error start to increase for very small h? What is the optimal step size for forward differences with double precision?","type":"content","url":"/exercises#self-assessment-questions","position":3},{"hierarchy":{"lvl1":"Exercises","lvl2":"Computational Exercises"},"type":"lvl2","url":"/exercises#computational-exercises","position":4},{"hierarchy":{"lvl1":"Exercises","lvl2":"Computational Exercises"},"content":"","type":"content","url":"/exercises#computational-exercises","position":5},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.1: Taylor Series Visualization","lvl2":"Computational Exercises"},"type":"lvl3","url":"/exercises#q1-1-taylor-series-visualization","position":6},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.1: Taylor Series Visualization","lvl2":"Computational Exercises"},"content":"Consider the program taylor.py or the equivalent Jupyter notebook posted on Moodle.\n\n(a) Setup your computer so that you can execute either program.\n\n(b) Create a plot showing the first four Taylor series expansions of f(x) = e^x:\n\nUse a different color for each line.\n\nPlot NumPy’s exponential function in dashed black.\n\nMake sure your name appears in the plot title.\n\nPut the absolute error of the approximation of e for each graph in the plot’s legend. Use two significant figures and scientific notation.\n\nHint\n\nUse np.exp(x) for the exact function. For Taylor polynomials, you can compute them iteratively by adding terms.","type":"content","url":"/exercises#q1-1-taylor-series-visualization","position":7},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.2: Taylor Polynomials of Quadratics","lvl2":"Computational Exercises"},"type":"lvl3","url":"/exercises#q1-2-taylor-polynomials-of-quadratics","position":8},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.2: Taylor Polynomials of Quadratics","lvl2":"Computational Exercises"},"content":"Consider the polynomial f(x) = x^2 - x - 2.\n\n(a) Find P_1(x), P_2(x) and P_3(x) for f(x) centered at x_0 = 0. What is the relation between P_3(x) and f(x)? Why?\n\n(b) Find P_1(x), P_2(x) and P_3(x) for f(x) centered at x_0 = 2. What is the relation between P_3(x) and f(x)? Why?\n\n(c) In general, given a polynomial f(x) with degree \\leq m, what can you say about f(x) - P_n(x) for n \\geq m?\n\nSolution\n\n(a) At x_0 = 0:\n\nf(0) = -2, f'(0) = -1, f''(0) = 2, f'''(0) = 0\n\nP_1(x) = -2 - x\n\nP_2(x) = -2 - x + x^2\n\nP_3(x) = -2 - x + x^2 = f(x)\n\nP_3(x) = f(x) because f is a degree 2 polynomial, and P_n(x) = f(x) for all n \\geq 2.\n\n(b) At x_0 = 2:\n\nf(2) = 0, f'(2) = 3, f''(2) = 2\n\nP_1(x) = 3(x-2)\n\nP_2(x) = 3(x-2) + (x-2)^2 = x^2 - x - 2 = f(x)\n\n(c) For any polynomial f of degree \\leq m and n \\geq m: f(x) - P_n(x) = 0.","type":"content","url":"/exercises#q1-2-taylor-polynomials-of-quadratics","position":9},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.3: Taylor Approximation of tan(x)","lvl2":"Computational Exercises"},"type":"lvl3","url":"/exercises#q1-3-taylor-approximation-of-tan-x","position":10},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.3: Taylor Approximation of tan(x)","lvl2":"Computational Exercises"},"content":"Find both P_2(x) and P_3(x) for f(x) = \\tan x about x_0 = 0, and use them to approximate \\tan(0.2).\n\nHint: Plots may be helpful to answer the following questions.\n\n(a) Show that in each case the remainder term provides an upper bound for the true error.\n\n(b) Using the remainder term, find a minimum value of k for P_k(x) to approximate f(x) to within 5 \\times 10^{-2} on [0, 0.5].\n\nSolution\n\nFor f(x) = \\tan x at x_0 = 0:\n\nf(0) = 0\n\nf'(x) = \\sec^2 x, so f'(0) = 1\n\nf''(x) = 2\\sec^2 x \\tan x, so f''(0) = 0\n\nf'''(x) = 2\\sec^4 x + 4\\sec^2 x \\tan^2 x, so f'''(0) = 2\n\nTherefore:\n\nP_2(x) = x\n\nP_3(x) = x + \\frac{x^3}{3}\n\nApproximations: P_2(0.2) = 0.2, P_3(0.2) = 0.2027\n\nThe true value is \\tan(0.2) \\approx 0.2027, so P_3 is much more accurate.","type":"content","url":"/exercises#q1-3-taylor-approximation-of-tan-x","position":11},{"hierarchy":{"lvl1":"Exercises","lvl2":"Finite Difference Exercises"},"type":"lvl2","url":"/exercises#finite-difference-exercises","position":12},{"hierarchy":{"lvl1":"Exercises","lvl2":"Finite Difference Exercises"},"content":"","type":"content","url":"/exercises#finite-difference-exercises","position":13},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.4: Central Difference Analysis","lvl2":"Finite Difference Exercises"},"type":"lvl3","url":"/exercises#q1-4-central-difference-analysis","position":14},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.4: Central Difference Analysis","lvl2":"Finite Difference Exercises"},"content":"Consider the central finite difference scheme to approximate f'(x_0):f'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0 - h)}{2h}\n\n(a) Show that the approximation error is E(h) = Ch^q, where C and q are constants that you will determine.\n\n(b) Consider the function f(x) = \\tan(x) at x_0 = 0.2. Develop a function that approximates f'(x_0) using the central difference scheme. Using that function, create a log-log plot of the absolute error against the step size h for h = 10^{-N}, N \\in [0, 20]. Also plot the function E(h) = Ch^q. Does the slope of this line match the slope of the absolute error? Explain what you observe.\n\nSolution\n\n(a) From the derivation in the notes: q = 2 and C = \\frac{1}{12}(f'''(\\xi^+) + f'''(\\xi^-)).\n\n(b) The log-log plot should show slope 2 for moderate values of h. For very small h, round-off error dominates and the error increases.","type":"content","url":"/exercises#q1-4-central-difference-analysis","position":15},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.5: Second Derivative Approximation","lvl2":"Finite Difference Exercises"},"type":"lvl3","url":"/exercises#q1-5-second-derivative-approximation","position":16},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.5: Second Derivative Approximation","lvl2":"Finite Difference Exercises"},"content":"Given a function f(x), use Taylor’s theorem to derive a second-order approximation of f''(x_0).\n\nHint: Your finite difference approximation for f''(x_0) should include the terms f(x_0 \\pm h) and f(x_0).\n\n(a) What is the precise form of the error term?\n\n(b) Create the following plots, and for each explain what you observe:\n\nPlot the value f''(1) against the discretization size h.\n\nPlot the absolute error vs. the discretization size h.\n\nPlot the absolute error divided by h^2 vs. the discretization size h. Verify that the values converge to the value derived from the error term.\n\nSolution\n\nThe second derivative approximation is:f''(x_0) \\approx \\frac{f(x_0 + h) - 2f(x_0) + f(x_0 - h)}{h^2}\n\nThe error is E(h) = -\\frac{h^2}{12}f^{(4)}(\\xi) for some \\xi near x_0.\n\nThis is a second-order method (\\mathcal{O}(h^2)).","type":"content","url":"/exercises#q1-5-second-derivative-approximation","position":17},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.6: One-Sided Second-Order Approximation","lvl2":"Finite Difference Exercises"},"type":"lvl3","url":"/exercises#q1-6-one-sided-second-order-approximation","position":18},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.6: One-Sided Second-Order Approximation","lvl2":"Finite Difference Exercises"},"content":"Given a function f(x), use Taylor approximations to derive a second-order (i.e., the error should be E(h) = Ch^2) approximation to f'(x_0).\n\nHint: Your approximation should include the terms f(x_0), f(x_0 + h), and f(x_0 + 2h).\n\n(a) What is the precise form of the error term?\n\n(b) Using your formula, approximate f'(0) where f(x) = \\sin(x) for h = 2^{-p}, p \\in [1, 15]. Create:\n\nA semilog plot of f'(0) against h.\n\nA log-log plot of the absolute error vs. h.\n\nA plot of the absolute error divided by h^2 vs. h.\n\nHint\n\nExpand f(x_0 + h) and f(x_0 + 2h) in Taylor series, then find coefficients a, b, c such that:af(x_0) + bf(x_0 + h) + cf(x_0 + 2h) = f'(x_0) + \\mathcal{O}(h^2)\n\nSolution\n\nThe formula is:f'(x_0) \\approx \\frac{-3f(x_0) + 4f(x_0+h) - f(x_0+2h)}{2h}\n\nThis is useful when you can only sample to one side of x_0 (e.g., at a boundary).","type":"content","url":"/exercises#q1-6-one-sided-second-order-approximation","position":19},{"hierarchy":{"lvl1":"Exercises","lvl2":"Taylor Series Practice"},"type":"lvl2","url":"/exercises#taylor-series-practice","position":20},{"hierarchy":{"lvl1":"Exercises","lvl2":"Taylor Series Practice"},"content":"","type":"content","url":"/exercises#taylor-series-practice","position":21},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.7: Computing Taylor Series","lvl2":"Taylor Series Practice"},"type":"lvl3","url":"/exercises#q1-7-computing-taylor-series","position":22},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q1.7: Computing Taylor Series","lvl2":"Taylor Series Practice"},"content":"Compute Taylor series for the following functions. Report the first three nonzero terms and show your work.\n\n(a) \\sqrt{x} at x = a (for a > 0)\n\n(b) x^{3/2} at x = a (for a > 0)\n\n(c) e^x at x = 0, 1, 2\n\n(d) \\cos(x) at x = 0, \\pi/4, \\pi/2\n\n(e) \\sin(x) at x = 0, \\pi/4, \\pi/2\n\n(f) \\tan(x) at x = 0, \\pi/4. Can you compute a Taylor series near x = \\pi/2? Why or why not?\n\nSolution for (f)\n\nAt x = 0: \\tan x = x + \\frac{x^3}{3} + \\frac{2x^5}{15} + \\cdots\n\nAt x = \\pi/4: \\tan x = 1 + 2(x - \\pi/4) + 2(x - \\pi/4)^2 + \\cdots\n\nAt x = \\pi/2: No Taylor series exists because \\tan(x) has a vertical asymptote at x = \\pi/2 (it is not continuous there, let alone differentiable).","type":"content","url":"/exercises#q1-7-computing-taylor-series","position":23},{"hierarchy":{"lvl1":"Approximation Theory"},"type":"lvl1","url":"/index-1","position":0},{"hierarchy":{"lvl1":"Approximation Theory"},"content":"Big Idea\n\nComputers can only do arithmetic. Everything else—e^x, \\sin(x), derivatives, integrals—must be approximated using additions and multiplications. Taylor’s theorem tells us how to do this and how much error we incur.","type":"content","url":"/index-1","position":1},{"hierarchy":{"lvl1":"Approximation Theory","lvl2":"Overview"},"type":"lvl2","url":"/index-1#overview","position":2},{"hierarchy":{"lvl1":"Approximation Theory","lvl2":"Overview"},"content":"Suppose you want to write a program to compute e^x, or solve a differential equation, or evaluate an integral. How would you do it?\n\nThe challenge is that computers are simple machines. They can only perform basic arithmetic: +, -, \\times, \\div. But scientific computing demands much more:\n\nHow do you compute e^x? You can’t—not exactly—using only arithmetic.\n\nHow do you compute \\sin(x), \\ln(x), \\sqrt{x}? Same problem.\n\nHow do you compute a derivative f'(x)? You’d need a limit—not a finite operation.\n\nHow do you compute an integral \\int_a^b f(x)\\,dx? Again, not directly computable.\n\nThe solution: Approximate these objects using only arithmetic. Taylor polynomials let us write:e^x \\approx 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6} + \\cdots\n\nNow we can compute the right-hand side—it’s just additions and multiplications. The cost is an approximation error. Taylor’s theorem tells us exactly how large that error is:f(x) = \\underbrace{f(a) + f'(a)(x-a) + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n}_{\\text{computable}} + \\underbrace{R_n(x)}_{\\text{error}}\n\nThis is the starting point for all of numerical analysis.","type":"content","url":"/index-1#overview","position":3},{"hierarchy":{"lvl1":"Approximation Theory","lvl2":"Learning Outcomes"},"type":"lvl2","url":"/index-1#learning-outcomes","position":4},{"hierarchy":{"lvl1":"Approximation Theory","lvl2":"Learning Outcomes"},"content":"After completing this chapter, you should be able to:\n\nL1.1: Write Taylor expansions with Lagrange remainder.\n\nL1.2: Derive finite difference formulas from Taylor series.\n\nL1.3: Analyze truncation error order (O(h), O(h^2), etc.).\n\nL1.4: Explain the trade-off between truncation and roundoff error.\n\nL1.5: Choose appropriate step sizes for numerical differentiation.","type":"content","url":"/index-1#learning-outcomes","position":5},{"hierarchy":{"lvl1":"Numerical Differentiation"},"type":"lvl1","url":"/numerical-differentiation","position":0},{"hierarchy":{"lvl1":"Numerical Differentiation"},"content":"Big Idea\n\nDerivatives can be approximated using function values at discrete points. Taylor’s theorem tells us exactly how accurate these approximations are and reveals the fundamental trade-off between truncation error and round-off error.","type":"content","url":"/numerical-differentiation","position":1},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"The Forward Difference Formula"},"type":"lvl2","url":"/numerical-differentiation#the-forward-difference-formula","position":2},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"The Forward Difference Formula"},"content":"Suppose we have a function f(x) and we want to compute its derivative at a point x_0 (i.e., the slope of the tangent line to the curve y = f(x) at x_0).\n\nRecall the limit definition of the derivative:f'(x_0) = \\lim_{h\\to 0} \\frac{f(x_0 + h) - f(x_0)}{h}\n\nThe left-hand side is the slope of the tangent line, while the right-hand side is the limit of the slope of the secant line connecting the points (x_0, f(x_0)) and (x_0 + h, f(x_0 + h)).\n\nThis suggests we can approximate the derivative using:f'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0)}{h}\n\nfor h “small”. A major question is: how accurate is this approximation?","type":"content","url":"/numerical-differentiation#the-forward-difference-formula","position":3},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl3":"Error Analysis","lvl2":"The Forward Difference Formula"},"type":"lvl3","url":"/numerical-differentiation#error-analysis","position":4},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl3":"Error Analysis","lvl2":"The Forward Difference Formula"},"content":"Forward Difference Error\n\nThe forward difference approximation satisfies:\\frac{f(x_0 + h) - f(x_0)}{h} = f'(x_0) + E(h)\n\nwhere the error is:E(h) = -\\frac{h}{2} f''(\\xi) = \\mathcal{O}(h)\n\nfor some \\xi \\in (x_0, x_0 + h). The error is first-order in h.\n\nUsing Taylor’s theorem with k = 1:f(x_0 + h) = f(x_0) + f'(x_0) h + \\frac{f''(\\xi)}{2} h^2\n\nwhere \\xi \\in (x_0, x_0 + h).\n\nSubstituting into our secant line approximation:\\begin{split}\n\\frac{f(x_0 + h) - f(x_0)}{h} &= \\frac{1}{h}\\left( f(x_0) + h f'(x_0) + \\frac{1}{2} h^2 f''(\\xi) - f(x_0) \\right) \\\\\n&= f'(x_0) + \\frac{1}{2} h f''(\\xi)\n\\end{split}\n\nComputational Verification\n\nTo verify the error theory: if E(h) = Ch, then \\log E = \\log C + \\log h.\n\nPlot \\log E vs. \\log h — you should see a straight line with slope 1.\n\n\n\nThe absolute error of the forward difference approximation of the derivative for \\sin(x_0) at x_0 = 1.2.","type":"content","url":"/numerical-differentiation#error-analysis","position":5},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"The Round-off Error Trade-off"},"type":"lvl2","url":"/numerical-differentiation#the-round-off-error-trade-off","position":6},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"The Round-off Error Trade-off"},"content":"The plot above shows something surprising: the error increases for very small h!\n\nThis is our first encounter with a fundamental phenomenon in scientific computing: round-off error. When h becomes very small, we’re subtracting two nearly equal numbers (f(x_0 + h) \\approx f(x_0)), and then dividing by a tiny h. This amplifies the small errors inherent in floating-point arithmetic.\n\nTip\n\nIn the \n\nnext chapter on floating-point arithmetic, we’ll learn:\n\nWhy computers can’t represent most real numbers exactly\n\nWhat machine epsilon is and why it’s approximately \n\n10-16 for double precision\n\nHow to find the optimal step size that balances truncation and round-off error\n\nFor now, the practical takeaway is: smaller h is not always better. For first-order finite differences, h \\approx 10^{-8} is often optimal.","type":"content","url":"/numerical-differentiation#the-round-off-error-trade-off","position":7},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"The Central Difference Formula"},"type":"lvl2","url":"/numerical-differentiation#the-central-difference-formula","position":8},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"The Central Difference Formula"},"content":"The forward difference only uses information to the right of x_0. From geometric intuition, using points on both sides should give a better approximation to the tangent line—the secant line through (x_0 - h, f(x_0 - h)) and (x_0 + h, f(x_0 + h)) is more symmetric.\n\nCentral Difference Approximation\n\nThe central difference approximation:f'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0 - h)}{2h}\n\nhas error E(h) = \\mathcal{O}(h^2) — second-order in h. Squaring h squares the error!\n\nWe expand both f(x_0 + h) and f(x_0 - h) using Taylor series:f(x_0 \\pm h) = f(x_0) \\pm h f'(x_0) + \\frac{h^2}{2}f''(x_0) \\pm \\frac{h^3}{6} f'''(\\xi^{\\pm}) + O(h^4)\n\nSubstituting into the central difference formula:\\begin{split}\n\\frac{f(x_0 + h) - f(x_0 - h)}{2h}\n&= \\frac{1}{2h}\\left( 2hf'(x_0) + \\frac{h^3}{6}\\left( f'''(\\xi^+) + f'''(\\xi^-) \\right) + O(h^5)\\right) \\\\\n&= f'(x_0) + \\frac{h^2}{12} \\left( f'''(\\xi^-) + f'''(\\xi^+) \\right) + O(h^4)\n\\end{split}\n\nWhy the improvement? The even-powered terms (h^2, h^4, \\ldots) cancel due to symmetry.\n\n\n\nCentral finite difference error for approximating \\tan'(0.2). The error follows O(h^2) (red line) until roundoff error dominates at h \\approx 10^{-6}.","type":"content","url":"/numerical-differentiation#the-central-difference-formula","position":9},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl3":"Comparison: Forward vs Central","lvl2":"The Central Difference Formula"},"type":"lvl3","url":"/numerical-differentiation#comparison-forward-vs-central","position":10},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl3":"Comparison: Forward vs Central","lvl2":"The Central Difference Formula"},"content":"Method\n\nFormula\n\nError\n\nOrder\n\nForward\n\n\\frac{f(x_0 + h) - f(x_0)}{h}\n\nO(h)\n\n1st\n\nCentral\n\n\\frac{f(x_0 + h) - f(x_0 - h)}{2h}\n\nO(h^2)\n\n2nd\n\nAccuracy Comparison\n\nFor h = 0.01:\n\nForward difference error: \\sim 0.01\n\nCentral difference error: \\sim 0.0001\n\nThe central difference is 100× more accurate for the same step size!\n\nOptimal Step Size for Central Differences\n\nThe total error (truncation + roundoff) is:E(h) = \\frac{M_0 \\mu}{h} + M_1 h^2\n\nMinimizing gives h_{\\text{opt}} \\approx \\mu^{1/3} \\approx 6 \\times 10^{-6} for double precision.\n\nThis is larger than for forward differences because truncation error decreases faster (h^2 vs h).","type":"content","url":"/numerical-differentiation#comparison-forward-vs-central","position":11},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"Second Derivative Approximation"},"type":"lvl2","url":"/numerical-differentiation#second-derivative-approximation","position":12},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"Second Derivative Approximation"},"content":"Second Derivative Formula\n\nThe central difference approximation for the second derivative:f''(x_0) \\approx \\frac{f(x_0 + h) - 2f(x_0) + f(x_0 - h)}{h^2}\n\nhas error O(h^2).\n\nAdding the Taylor expansions (instead of subtracting):f(x_0 + h) + f(x_0 - h) = 2f(x_0) + h^2 f''(x_0) + O(h^4)\n\nSolving for f''(x_0) gives the formula above.\n\nThis formula is fundamental in numerical PDEs—it’s the discrete Laplacian in one dimension.","type":"content","url":"/numerical-differentiation#second-derivative-approximation","position":13},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"Summary"},"type":"lvl2","url":"/numerical-differentiation#summary","position":14},{"hierarchy":{"lvl1":"Numerical Differentiation","lvl2":"Summary"},"content":"Method\n\nFormula\n\nError Order\n\nForward difference\n\n\\frac{f(x+h) - f(x)}{h}\n\nO(h)\n\nCentral difference\n\n\\frac{f(x+h) - f(x-h)}{2h}\n\nO(h^2)\n\nSecond derivative\n\n\\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}\n\nO(h^2)\n\nKey principle: Using symmetric stencils improves accuracy, but there are diminishing returns due to round-off error. The optimal step size balances truncation and round-off errors.","type":"content","url":"/numerical-differentiation#summary","position":15},{"hierarchy":{"lvl1":"Numerical Integration"},"type":"lvl1","url":"/numerical-integration","position":0},{"hierarchy":{"lvl1":"Numerical Integration"},"content":"Big Idea\n\nIntegrals can be approximated by replacing the integrand with simple functions (polynomials) and integrating those exactly. Unlike differentiation, integration is a smoothing operation—errors tend to average out rather than amplify.","type":"content","url":"/numerical-integration","position":1},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"The Trapezoidal Rule"},"type":"lvl2","url":"/numerical-integration#the-trapezoidal-rule","position":2},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"The Trapezoidal Rule"},"content":"The simplest approach: approximate f(x) by a straight line and integrate that.","type":"content","url":"/numerical-integration#the-trapezoidal-rule","position":3},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"Single Interval","lvl2":"The Trapezoidal Rule"},"type":"lvl3","url":"/numerical-integration#single-interval","position":4},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"Single Interval","lvl2":"The Trapezoidal Rule"},"content":"Trapezoidal Rule (Single Interval)\\int_a^b f(x)\\,dx \\approx \\frac{b-a}{2}\\left(f(a) + f(b)\\right)\n\nConnect (a, f(a)) to (b, f(b)) with a line; compute the trapezoid’s area.","type":"content","url":"/numerical-integration#single-interval","position":5},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"Local Error Analysis","lvl2":"The Trapezoidal Rule"},"type":"lvl3","url":"/numerical-integration#local-error-analysis","position":6},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"Local Error Analysis","lvl2":"The Trapezoidal Rule"},"content":"What is the error of this approximation? Let h = b - a and use Taylor’s theorem.\n\nLocal Truncation Error for Trapezoidal Rule\n\nFor f \\in C^2[a, b], the local truncation error (error on a single interval) is:\\int_a^b f(x)\\,dx - \\frac{h}{2}\\left(f(a) + f(b)\\right) = -\\frac{h^3}{12}f''(\\xi)\n\nfor some \\xi \\in (a, b), where h = b - a.\n\nExpand f(x) around the midpoint c = (a+b)/2 using Taylor’s theorem:f(x) = f(c) + f'(c)(x - c) + \\frac{f''(\\eta(x))}{2}(x - c)^2\n\nIntegrating from a to b:\\int_a^b f(x)\\,dx = hf(c) + \\frac{f''(\\eta)}{2}\\int_a^b (x-c)^2\\,dx = hf(c) + \\frac{h^3}{24}f''(\\eta)\n\nwhere we used that \\int_a^b (x-c)\\,dx = 0 by symmetry, and \\int_a^b (x-c)^2\\,dx = h^3/12.\n\nFor the trapezoidal approximation, expand f(a) and f(b) around c:f(a) = f(c) - \\frac{h}{2}f'(c) + \\frac{h^2}{8}f''(\\xi_1)f(b) = f(c) + \\frac{h}{2}f'(c) + \\frac{h^2}{8}f''(\\xi_2)\n\nAdding:\\frac{h}{2}(f(a) + f(b)) = hf(c) + \\frac{h^3}{16}\\cdot\\frac{f''(\\xi_1) + f''(\\xi_2)}{2}\n\nTaking the difference and using the intermediate value theorem to combine the f'' terms:\\int_a^b f(x)\\,dx - \\frac{h}{2}(f(a) + f(b)) = -\\frac{h^3}{12}f''(\\xi)\n\nfor some \\xi \\in (a, b).\n\nKey observation: The local error is O(h^3)—cubic in the interval width.","type":"content","url":"/numerical-integration#local-error-analysis","position":7},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Composite Trapezoidal Rule"},"type":"lvl2","url":"/numerical-integration#composite-trapezoidal-rule","position":8},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Composite Trapezoidal Rule"},"content":"For better accuracy, divide [a, b] into n subintervals of equal width h = (b-a)/n, with nodes x_k = a + kh for k = 0, 1, \\ldots, n.\n\nComposite Trapezoidal RuleT_n(f) = h\\left(\\frac{f(x_0) + f(x_n)}{2} + \\sum_{k=1}^{n-1} f(x_k)\\right)\n\nThis applies the trapezoidal rule to each subinterval and sums the results.","type":"content","url":"/numerical-integration#composite-trapezoidal-rule","position":9},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"From Local to Global Error","lvl2":"Composite Trapezoidal Rule"},"type":"lvl3","url":"/numerical-integration#from-local-to-global-error","position":10},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"From Local to Global Error","lvl2":"Composite Trapezoidal Rule"},"content":"The global error is the total error when approximating the integral over [a, b].\n\nGlobal Error for Composite Trapezoidal Rule\n\nFor f \\in C^2[a, b]:\\int_a^b f(x)\\,dx - T_n(f) = -\\frac{(b-a)h^2}{12}f''(\\xi) = O(h^2)\n\nfor some \\xi \\in (a, b).\n\nOn each subinterval [x_{k-1}, x_k], the local error is:\\int_{x_{k-1}}^{x_k} f(x)\\,dx - \\frac{h}{2}(f(x_{k-1}) + f(x_k)) = -\\frac{h^3}{12}f''(\\xi_k)\n\nfor some \\xi_k \\in (x_{k-1}, x_k).\n\nSumming over all n subintervals:\\int_a^b f(x)\\,dx - T_n(f) = -\\frac{h^3}{12}\\sum_{k=1}^{n} f''(\\xi_k)\n\nSince f'' \\in C[a,b], the sum \\frac{1}{n}\\sum_{k=1}^{n} f''(\\xi_k) lies between \\min f'' and \\max f''. By the intermediate value theorem, there exists \\xi \\in (a, b) such that:\\frac{1}{n}\\sum_{k=1}^{n} f''(\\xi_k) = f''(\\xi)\n\nTherefore:\\int_a^b f(x)\\,dx - T_n(f) = -\\frac{h^3}{12} \\cdot n \\cdot f''(\\xi) = -\\frac{h^3 n}{12}f''(\\xi)\n\nSince n = (b-a)/h:\\int_a^b f(x)\\,dx - T_n(f) = -\\frac{(b-a)h^2}{12}f''(\\xi)","type":"content","url":"/numerical-integration#from-local-to-global-error","position":11},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"Understanding Local vs Global Error","lvl2":"Composite Trapezoidal Rule"},"type":"lvl3","url":"/numerical-integration#understanding-local-vs-global-error","position":12},{"hierarchy":{"lvl1":"Numerical Integration","lvl3":"Understanding Local vs Global Error","lvl2":"Composite Trapezoidal Rule"},"content":"Error Type\n\nDefinition\n\nTrapezoidal Rule\n\nLocal\n\nError on one subinterval of width h\n\nO(h^3)\n\nGlobal\n\nTotal error over [a, b]\n\nO(h^2)\n\nWhy does the order drop from 3 to 2?\n\nThe global error accumulates local errors from n \\sim 1/h subintervals:\\text{Global error} \\sim n \\times \\text{Local error} \\sim \\frac{1}{h} \\times h^3 = h^2\n\nThis is the typical pattern: global order = local order − 1.\n\nPython Implementationdef trapezoidal(f, a, b, n):\n    \"\"\"Composite trapezoidal rule with n subintervals.\"\"\"\n    h = (b - a) / n\n    x = np.linspace(a, b, n + 1)\n    y = f(x)\n    return h * (0.5 * y[0] + np.sum(y[1:-1]) + 0.5 * y[-1])","type":"content","url":"/numerical-integration#understanding-local-vs-global-error","position":13},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Higher-Order Methods"},"type":"lvl2","url":"/numerical-integration#higher-order-methods","position":14},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Higher-Order Methods"},"content":"By using higher-degree polynomial approximations, we can achieve better accuracy.\n\nSimpson’s rule uses a quadratic polynomial through three points (a, f(a)), (m, f(m)), (b, f(b)) where m = (a+b)/2:\\int_a^b f(x)\\,dx \\approx \\frac{h}{6}\\left(f(a) + 4f(m) + f(b)\\right)\n\nwhere h = b - a. This has local error O(h^5) and global error O(h^4)—two orders better than trapezoidal.\n\nEven higher-order Newton-Cotes formulas exist (using more equally-spaced points), though they become unstable for high orders. The optimal approach—Gaussian quadrature—chooses both the nodes and weights optimally and achieves remarkable efficiency. We will explore this in the chapter on \n\ninterpolation.","type":"content","url":"/numerical-integration#higher-order-methods","position":15},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Why Integration is Easier Than Differentiation"},"type":"lvl2","url":"/numerical-integration#why-integration-is-easier-than-differentiation","position":16},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Why Integration is Easier Than Differentiation"},"content":"Smoothing vs. Roughening\n\nOperation\n\nError behavior\n\nDifferentiation\n\nErrors amplify (dividing by small h)\n\nIntegration\n\nErrors average out (summing many terms)\n\nThis is why numerical integration is generally more stable than numerical differentiation. Integration “smooths,” differentiation “roughens.”\n\nFrom the perspective of conditioning:\n\nDifferentiation amplifies high-frequency noise\n\nIntegration damps high-frequency components\n\nThis is why we can often integrate noisy data reliably, but differentiating noisy data is notoriously difficult.","type":"content","url":"/numerical-integration#why-integration-is-easier-than-differentiation","position":17},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Summary"},"type":"lvl2","url":"/numerical-integration#summary","position":18},{"hierarchy":{"lvl1":"Numerical Integration","lvl2":"Summary"},"content":"Rule\n\nLocal Error\n\nGlobal Error\n\nTrapezoidal\n\nO(h^3)\n\nO(h^2)\n\nSimpson’s\n\nO(h^5)\n\nO(h^4)\n\nKey principle: Global order = local order − 1, because we sum O(1/h) local errors.","type":"content","url":"/numerical-integration#summary","position":19},{"hierarchy":{"lvl1":"Taylor’s Theorem"},"type":"lvl1","url":"/taylor","position":0},{"hierarchy":{"lvl1":"Taylor’s Theorem"},"content":"Big Idea\n\nUse Taylor’s theorem to derive finite difference approximations for derivatives and analyze their accuracy. Understanding the interplay between approximation error and round-off error is fundamental to scientific computing.","type":"content","url":"/taylor","position":1},{"hierarchy":{"lvl1":"Taylor’s Theorem","lvl2":"Statement of Taylor’s Theorem"},"type":"lvl2","url":"/taylor#statement-of-taylors-theorem","position":2},{"hierarchy":{"lvl1":"Taylor’s Theorem","lvl2":"Statement of Taylor’s Theorem"},"content":"Taylor’s Theorem\n\nAssume that f \\in C^{k+1}(a, b) (i.e., f(x) has k + 1 continuous derivatives). Then for x_0 \\in (a, b):f(x_0 + h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2} f''(x_0)\n     + \\cdots + \\frac{h^k}{k!} f^{(k)}(x_0)\n     + \\frac{h^{k+1}}{(k+1)!} f^{(k+1)}(\\xi),\n\nwhere \\xi \\in (x_0, x_0 + h).\n\nSuppose we have a function f(x) with k + 1 continuous derivatives. Then we can write:\\begin{split}\nf(x) &= \\underbrace{P_k(x)}_{\\text{$k$-th Taylor polynomial}} +\n        \\underbrace{R_k(x)}_{\\text{Remainder or error term}} \\\\\n     &= \\sum_{i=0}^{k} \\frac{f^{(i)}(x_0)}{i!} (x - x_0)^i +\n        \\frac{f^{(k+1)}(\\xi)}{(k+1)!}(x - x_0)^{k+1},\n\\end{split}\n\nwhere \\xi \\in (x_0, x_0 + h). This means that:|f(x) - P_k(x)| = |R_k(x)| \\quad\\implies\\quad\n\\sup_{x \\in [a, b]} |f(x) - P_k(x)| = \\sup_{x \\in [a, b]} |R_k(x)|.\n\nKey Properties of Taylor Polynomials\n\nWhen we let k \\to \\infty, P_k(x) becomes the Taylor series.\n\nGiven the polynomial P_k(x), we have:P_k(x_0) = f(x_0), \\quad P_k'(x_0) = f'(x_0), \\quad P_k''(x_0) = f''(x_0), \\quad \\ldots\n\nIn fact, P_k(x) is the unique polynomial of degree \\leq k such that P_k^{(n)}(x_0) = f^{(n)}(x_0) for n = 0, 1, \\ldots, k.\n\nComputing e to a Given Accuracy\n\nSuppose we want to compute the number e to an accuracy of \n\n10-3.\n\nWe can do this by computing a Taylor series for the function e^x. Since we know the value at x = 0, namely f(0) = e^0 = 1, we use that as the point about which we create the Taylor series.\n\nSince f^{(n)}(x) = e^x, we have f^{(n)}(0) = 1 for all n.\n\nThe n-degree Taylor polynomial is:T_n(x) = 1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\n\nTo estimate the error, we compute the remainder:R_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}x^{n+1}\n\nwhere \\xi \\in [0, 1]. Since e^x is positive and increasing:\\sup_{x \\in [0, 1]}|R_n(x)| \\leq \\frac{e^x}{(n+1)!} \\leq \\frac{3}{(n+1)!}.\n\nTo achieve the required accuracy:\\sup_{x \\in [0, 1]}|R_n(x)| \\leq 10^{-3} \\quad\\implies\\quad \\frac{3}{(n+1)!} \\leq 10^{-3}.\n\nWith some experimentation, we find that the first n for which this inequality holds is n = 6.\n\n\n\nThe first four Taylor polynomials P_1, P_2, P_3, P_4 of f(x) = e^x centered at x_0 = 0. The approximations converge to the true function (dashed black) as the degree increases.","type":"content","url":"/taylor#statement-of-taylors-theorem","position":3},{"hierarchy":{"lvl1":"Taylor’s Theorem","lvl2":"Why Taylor’s Theorem Matters"},"type":"lvl2","url":"/taylor#why-taylors-theorem-matters","position":4},{"hierarchy":{"lvl1":"Taylor’s Theorem","lvl2":"Why Taylor’s Theorem Matters"},"content":"Taylor’s theorem is a workhorse of numerical analysis because:\n\nIt allows us to replace complicated functions with polynomials\n\nThe remainder term gives us explicit error bounds\n\nIt reveals how accuracy depends on the step size h\n\nIn the next section, we’ll use Taylor’s theorem to derive finite difference approximations for derivatives.","type":"content","url":"/taylor#why-taylors-theorem-matters","position":5},{"hierarchy":{"lvl1":"Condition Numbers"},"type":"lvl1","url":"/condition-numbers","position":0},{"hierarchy":{"lvl1":"Condition Numbers"},"content":"Big Idea\n\nThe condition number measures how sensitive a problem is to small perturbations in the input. Large condition numbers signal potential trouble regardless of the algorithm used—it’s a property of the mathematics, not the implementation.","type":"content","url":"/condition-numbers","position":1},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Why Condition Numbers Matter"},"type":"lvl2","url":"/condition-numbers#why-condition-numbers-matter","position":2},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Why Condition Numbers Matter"},"content":"Before we discuss algorithms and their errors, we need to understand: some problems are inherently harder than others.\n\nConsider computing f(x) = \\tan(x) near x = \\pi/2. Even with infinite precision arithmetic and a perfect algorithm, tiny uncertainties in x cause huge changes in \\tan(x). This isn’t a flaw in our algorithm—it’s the nature of the tangent function near its pole.\n\nThe condition number quantifies this sensitivity. It answers: if my input is slightly wrong, how wrong might my output be?","type":"content","url":"/condition-numbers#why-condition-numbers-matter","position":3},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Absolute and Relative Error"},"type":"lvl2","url":"/condition-numbers#absolute-and-relative-error","position":4},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Absolute and Relative Error"},"content":"Before measuring problem sensitivity, we need precise definitions of error.\n\nAbsolute and Relative Error\n\nLet x be the true value and x^* be a numerical approximation.\n\nAbsolute error:\\text{Abs}(x) = |x - x^*|\n\nRelative error:\\text{Rel}(x) = \\frac{|x - x^*|}{|x|}, \\quad x \\neq 0","type":"content","url":"/condition-numbers#absolute-and-relative-error","position":5},{"hierarchy":{"lvl1":"Condition Numbers","lvl3":"Why Relative Error Matters","lvl2":"Absolute and Relative Error"},"type":"lvl3","url":"/condition-numbers#why-relative-error-matters","position":6},{"hierarchy":{"lvl1":"Condition Numbers","lvl3":"Why Relative Error Matters","lvl2":"Absolute and Relative Error"},"content":"Same Absolute Error, Different Quality\n\n\n\nx = 1, x^* = 2\n\nx = 10^6, x^* = 10^6 + 1\n\nAbsolute error\n\n1\n\n1\n\nRelative error\n\n100\\%\n\n0.0001\\%\n\nQuality\n\nTerrible\n\nExcellent\n\nBoth have the same absolute error, but the first is off by 100% while the second is nearly perfect. Relative error captures what matters.","type":"content","url":"/condition-numbers#why-relative-error-matters","position":7},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Condition of f at x"},"type":"lvl2","url":"/condition-numbers#condition-of-f-at-x","position":8},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Condition of f at x"},"content":"When we want to evaluate f(x), even if x^* is close to x, the value f(x^*) may be far from f(x). The condition number quantifies this sensitivity.\n\nCondition Number\n\nGiven a function f(x) and a fixed location x, the condition number is:\\kappa := \\sup_{|x - x^*| \\neq 0} \\left| \\frac{\\text{Rel}(f(x))}{\\text{Rel}(x)} \\right| = \\sup_{|x - x^*| \\neq 0} \\left| \\frac{\\frac{f(x) - f(x^*)}{f(x)}}{\\frac{x - x^*}{x}} \\right|\n\nIf \\kappa \\gg 1 (large), then evaluating f at x is ill-conditioned\n\nIf \\kappa \\approx \\mathcal{O}(1), then evaluating f at x is well-conditioned","type":"content","url":"/condition-numbers#condition-of-f-at-x","position":9},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Simplified Formula via Taylor’s Theorem"},"type":"lvl2","url":"/condition-numbers#simplified-formula-via-taylors-theorem","position":10},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Simplified Formula via Taylor’s Theorem"},"content":"Condition Number Formula\n\nFor a differentiable function f, the condition number at x is:\\kappa = \\left| \\frac{x f'(x)}{f(x)} \\right|\n\nSince x^* is close to x, we can write x^* = x + h and apply Taylor’s theorem:f(x^*) = f(x + h) \\approx f(x) + f'(x)(x - x^*)\n\nThis gives us:f(x) - f(x^*) \\approx f'(x)(x - x^*)\n\nSubstituting into the condition number definition:\\kappa = \\sup_x \\left| \\frac{x f'(x)}{f(x)} \\right|","type":"content","url":"/condition-numbers#simplified-formula-via-taylors-theorem","position":11},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Examples"},"type":"lvl2","url":"/condition-numbers#examples","position":12},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Examples"},"content":"Square Root (Well-Conditioned)\n\nConsider f(x) = \\sqrt{x} with f'(x) = \\frac{1}{2\\sqrt{x}}.\n\nNear \\bar{x} = 1, we have \\bar{y} = f(\\bar{x}) = 1. Using a Taylor approximation:y - \\bar{y} = \\sqrt{x} - 1 \\approx \\frac{1}{2}(x - 1) = \\frac{1}{2}(x - \\bar{x})\n\nVariations in y are always smaller than variations in x. Computing the condition number:\\kappa = \\left| \\frac{x f'(x)}{f(x)} \\right| = \\left| \\frac{x \\cdot \\frac{1}{2\\sqrt{x}}}{\\sqrt{x}} \\right| = \\frac{1}{2}\n\nResult: \\kappa = 1/2 — evaluating \\sqrt{x} is well-conditioned.\n\nTangent Near π/2 (Ill-Conditioned)\n\nConsider f(x) = \\tan(x) near x = \\frac{\\pi}{2}.\n\nTake two points:x_1 = \\frac{\\pi}{2} - 0.001, \\quad x_2 = \\frac{\\pi}{2} - 0.002\n\nThen |x_1 - x_2| = 0.001, but |f(x_1) - f(x_2)| \\approx 500. A tiny input change causes a huge output change!\n\nComputing the condition number:\\kappa = \\left| \\frac{x f'(x)}{f(x)} \\right| = \\left| \\frac{x}{\\cos(x)\\sin(x)} \\right| = |2x \\csc(2x)| \\to \\infty \\text{ as } x \\to \\pi/2\n\nResult: \\kappa \\to \\infty — evaluating \\tan(x) near \\frac{\\pi}{2} is ill-conditioned.\n\nLogarithm Near 1 (Ill-Conditioned)\n\nConsider f(x) = \\ln(x) near x = 1.\\kappa = \\left| \\frac{x f'(x)}{f(x)} \\right| = \\left| \\frac{x \\cdot (1/x)}{\\ln(x)} \\right| = \\frac{1}{|\\ln(x)|}\n\nAs x \\to 1, we have \\ln(x) \\to 0, so \\kappa \\to \\infty.\n\nResult: \\kappa \\to \\infty — evaluating \\ln(x) near x = 1 is ill-conditioned.","type":"content","url":"/condition-numbers#examples","position":13},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Summary"},"type":"lvl2","url":"/condition-numbers#summary","position":14},{"hierarchy":{"lvl1":"Condition Numbers","lvl2":"Summary"},"content":"Problem\n\nCondition Number\n\nWell/Ill-Conditioned\n\nf(x) = \\sqrt{x}\n\n\\kappa = 1/2\n\nWell-conditioned\n\nf(x) = x^2\n\n\\kappa = 2\n\nWell-conditioned\n\nf(x) = \\tan(x) near \\pi/2\n\n\\kappa \\to \\infty\n\nIll-conditioned\n\nf(x) = \\ln(x) near 1\n\n\\kappa \\to \\infty\n\nIll-conditioned\n\nThe condition number is a property of the mathematical problem—it tells us the best possible accuracy any algorithm could achieve. But even well-conditioned problems can give bad results if we use a bad algorithm. That’s the subject of the \n\nnext section.","type":"content","url":"/condition-numbers#summary","position":15},{"hierarchy":{"lvl1":"Exercises"},"type":"lvl1","url":"/exercises-1","position":0},{"hierarchy":{"lvl1":"Exercises"},"content":"Practice analyzing errors, stability, and conditioning.","type":"content","url":"/exercises-1","position":1},{"hierarchy":{"lvl1":"Exercises","lvl2":"Self-Assessment Questions"},"type":"lvl2","url":"/exercises-1#self-assessment-questions","position":2},{"hierarchy":{"lvl1":"Exercises","lvl2":"Self-Assessment Questions"},"content":"Test your understanding with these conceptual questions:\n\nMachine Epsilon: What happens when you add a number smaller than \\varepsilon_{\\text{mach}} to 1.0?\n\nConditioning: What makes evaluating \\tan(x) near x = \\pi/2 ill-conditioned? Can any algorithm fix this?\n\nStability: Why is computing \\sqrt{x+1} - \\sqrt{x} directly unstable for large x? What’s a stable alternative?\n\nMachine Epsilon: What is the approximate machine epsilon for single and double precision? What does it tell us about the accuracy of floating point computations?\n\nBinary Representation: How do you convert a positive integer to its negative two’s complement representation?\n\nSubtractive Cancellation: When subtracting two numbers a and b, under what conditions is the result reliable?\n\nForward vs. Backward: If an algorithm has small backward error but large forward error, what does this tell you about the problem?\n\nTrade-offs: In finite differences, why can’t we just use an extremely small h to get arbitrary accuracy?","type":"content","url":"/exercises-1#self-assessment-questions","position":3},{"hierarchy":{"lvl1":"Exercises","lvl2":"Floating-Point Arithmetic"},"type":"lvl2","url":"/exercises-1#floating-point-arithmetic","position":4},{"hierarchy":{"lvl1":"Exercises","lvl2":"Floating-Point Arithmetic"},"content":"","type":"content","url":"/exercises-1#floating-point-arithmetic","position":5},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.1: Machine Epsilon","lvl2":"Floating-Point Arithmetic"},"type":"lvl3","url":"/exercises-1#q2-1-machine-epsilon","position":6},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.1: Machine Epsilon","lvl2":"Floating-Point Arithmetic"},"content":"(a) Write a program to experimentally determine machine epsilon for single (32-bit) and double (64-bit) precision.\n\n(b) Verify your answers against the theoretical values \n\n2-24 and \n\n2-53.\n\n(c) Compute 1 + \\varepsilon_{\\text{mach}}/2 in double precision. What do you get? Why?","type":"content","url":"/exercises-1#q2-1-machine-epsilon","position":7},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.2: Representation Errors","lvl2":"Floating-Point Arithmetic"},"type":"lvl3","url":"/exercises-1#q2-2-representation-errors","position":8},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.2: Representation Errors","lvl2":"Floating-Point Arithmetic"},"content":"(a) Show that 0.1 cannot be represented exactly in binary floating-point.\n\n(b) Compute 0.1 + 0.1 + 0.1 - 0.3 in Python. Explain the result.\n\n(c) Why might sum([0.1] * 10) not equal 1.0?","type":"content","url":"/exercises-1#q2-2-representation-errors","position":9},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.3: Summation Order","lvl2":"Floating-Point Arithmetic"},"type":"lvl3","url":"/exercises-1#q2-3-summation-order","position":10},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.3: Summation Order","lvl2":"Floating-Point Arithmetic"},"content":"Consider summing s = \\sum_{k=1}^{n} \\frac{1}{k} for n = 10^7.\n\n(a) Compute the sum from k = 1 to n (forward).\n\n(b) Compute the sum from k = n to 1 (backward).\n\n(c) Which is more accurate? Why? (Hint: think about the relative sizes of terms being added.)","type":"content","url":"/exercises-1#q2-3-summation-order","position":11},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.4: Kahan Summation","lvl2":"Floating-Point Arithmetic"},"type":"lvl3","url":"/exercises-1#q2-4-kahan-summation","position":12},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.4: Kahan Summation","lvl2":"Floating-Point Arithmetic"},"content":"Kahan (1965) proposed a compensated summation algorithm that tracks roundoff error:def kahan_sum(xs):\n    s = 0.0  # Running sum\n    c = 0.0  # Compensation for lost low-order bits\n\n    for x in xs:\n        y = x - c        # Compensate for previous error\n        t = s + y        # Add to sum\n        c = (t - s) - y  # Recover the roundoff error\n        s = t\n\n    return s\n\n(a) Implement this algorithm.\n\n(b) Test with the sum S = 10000 + \\pi + e using 6-digit decimal arithmetic. Trace through by hand.\n\n(c) What is the role of the variable c? Why does it improve accuracy?\n\n(d) Compare Kahan summation to naive summation for the integral:\nI = \\int_1^{2^{22}} \\frac{dx}{\\sqrt{x}} = 2(2^{11} - 1)\n\n\nusing the trapezoidal rule with N = 10^6 points, in single precision (np.float32).","type":"content","url":"/exercises-1#q2-4-kahan-summation","position":13},{"hierarchy":{"lvl1":"Exercises","lvl2":"Forward and Backward Error"},"type":"lvl2","url":"/exercises-1#forward-and-backward-error","position":14},{"hierarchy":{"lvl1":"Exercises","lvl2":"Forward and Backward Error"},"content":"","type":"content","url":"/exercises-1#forward-and-backward-error","position":15},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.5: Quadratic Formula","lvl2":"Forward and Backward Error"},"type":"lvl3","url":"/exercises-1#q2-5-quadratic-formula","position":16},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.5: Quadratic Formula","lvl2":"Forward and Backward Error"},"content":"For ax^2 + bx + c = 0 with a = 1, b = -10^8, c = 1:\n\n(a) Compute both roots using the standard quadratic formula.\n\n(b) Identify which root suffers from cancellation.\n\n(c) Use the identity x_1 x_2 = c/a to compute the small root accurately.\n\n(d) Compute the backward error (residual) for both methods.","type":"content","url":"/exercises-1#q2-5-quadratic-formula","position":17},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.6: Exponential Minus One","lvl2":"Forward and Backward Error"},"type":"lvl3","url":"/exercises-1#q2-6-exponential-minus-one","position":18},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.6: Exponential Minus One","lvl2":"Forward and Backward Error"},"content":"(a) For x = 10^{-k}, k = 1, \\ldots, 16, compute e^x - 1 using direct subtraction.\n\n(b) Compare to numpy.expm1(x).\n\n(c) Plot the relative error of the direct method vs. x.\n\n(d) At what x does the direct method lose all significant digits?","type":"content","url":"/exercises-1#q2-6-exponential-minus-one","position":19},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.7: Variance Computation","lvl2":"Forward and Backward Error"},"type":"lvl3","url":"/exercises-1#q2-7-variance-computation","position":20},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.7: Variance Computation","lvl2":"Forward and Backward Error"},"content":"Consider computing variance: \\sigma^2 = \\frac{1}{n}\\sum(x_i - \\bar{x})^2.\n\nAn alternative “one-pass” formula is: \\sigma^2 = \\frac{1}{n}\\sum x_i^2 - \\bar{x}^2.\n\n(a) For x = [10^8, 10^8 + 1, 10^8 + 2], compute variance both ways.\n\n(b) Which method is more accurate? Why?\n\n(c) Research Welford’s algorithm. Why is it preferred?","type":"content","url":"/exercises-1#q2-7-variance-computation","position":21},{"hierarchy":{"lvl1":"Exercises","lvl2":"Condition Numbers"},"type":"lvl2","url":"/exercises-1#condition-numbers","position":22},{"hierarchy":{"lvl1":"Exercises","lvl2":"Condition Numbers"},"content":"","type":"content","url":"/exercises-1#condition-numbers","position":23},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.8: Condition Number Computation","lvl2":"Condition Numbers"},"type":"lvl3","url":"/exercises-1#q2-8-condition-number-computation","position":24},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.8: Condition Number Computation","lvl2":"Condition Numbers"},"content":"Compute the condition number \\kappa = |xf'(x)/f(x)| for:\n\n(a) f(x) = e^x at x = 1\n\n(b) f(x) = \\ln(x) at x = 1 and x = e\n\n(c) f(x) = \\sin(x) at x = 0 and x = \\pi\n\n(d) f(x) = x^n at x = 1 for integer n\n\nWhich evaluations are well-conditioned? Which are ill-conditioned?\n\nSolution for (a)\n\nFor f(x) = e^x, we have f'(x) = e^x.K = \\left|\\frac{x f'(x)}{f(x)}\\right| = \\left|\\frac{x e^x}{e^x}\\right| = |x|\n\nAt x = 1: K = 1. This is well-conditioned.","type":"content","url":"/exercises-1#q2-8-condition-number-computation","position":25},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.9: Subtractive Cancellation","lvl2":"Condition Numbers"},"type":"lvl3","url":"/exercises-1#q2-9-subtractive-cancellation","position":26},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.9: Subtractive Cancellation","lvl2":"Condition Numbers"},"content":"The expression \\sqrt{x+1} - \\sqrt{x} suffers from cancellation for large x.\n\n(a) Compute this directly for x = 10^{16}.\n\n(b) Rationalize to get \\frac{1}{\\sqrt{x+1} + \\sqrt{x}} and compute again.\n\n(c) What is the relative error of the direct method?","type":"content","url":"/exercises-1#q2-9-subtractive-cancellation","position":27},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.10: Trigonometric Reformulation","lvl2":"Condition Numbers"},"type":"lvl3","url":"/exercises-1#q2-10-trigonometric-reformulation","position":28},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.10: Trigonometric Reformulation","lvl2":"Condition Numbers"},"content":"For small x, the expression 1 - \\cos(x) loses accuracy.\n\n(a) Compute 1 - \\cos(10^{-8}) directly.\n\n(b) Use the identity 1 - \\cos(x) = 2\\sin^2(x/2) and compute again.\n\n(c) Compare to the Taylor approximation 1 - \\cos(x) \\approx x^2/2.","type":"content","url":"/exercises-1#q2-10-trigonometric-reformulation","position":29},{"hierarchy":{"lvl1":"Exercises","lvl2":"Stability Analysis"},"type":"lvl2","url":"/exercises-1#stability-analysis","position":30},{"hierarchy":{"lvl1":"Exercises","lvl2":"Stability Analysis"},"content":"","type":"content","url":"/exercises-1#stability-analysis","position":31},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.11: Stable Reformulation","lvl2":"Stability Analysis"},"type":"lvl3","url":"/exercises-1#q2-11-stable-reformulation","position":32},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.11: Stable Reformulation","lvl2":"Stability Analysis"},"content":"For each of the following expressions, identify the potential numerical issue and propose a stable reformulation:\n\n(a) \\sqrt{x^2 + 1} - x for large positive x\n\n(b) \\frac{1 - \\cos(x)}{x^2} for small x\n\n(c) e^x - 1 for small x\n\n(d) \\ln(x) - \\ln(y) when x \\approx y\n\nSolution for (a)\n\nProblem: Subtractive cancellation when \\sqrt{x^2+1} \\approx x.\n\nStable form: Multiply by \\frac{\\sqrt{x^2+1}+x}{\\sqrt{x^2+1}+x}:\\sqrt{x^2+1} - x = \\frac{(x^2+1) - x^2}{\\sqrt{x^2+1}+x} = \\frac{1}{\\sqrt{x^2+1}+x}","type":"content","url":"/exercises-1#q2-11-stable-reformulation","position":33},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.12: Residuals and Forward Error","lvl2":"Stability Analysis"},"type":"lvl3","url":"/exercises-1#q2-12-residuals-and-forward-error","position":34},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.12: Residuals and Forward Error","lvl2":"Stability Analysis"},"content":"Solve the system:\\begin{pmatrix} 1 & 1 \\\\ 1 & 1.0001 \\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} =\n\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n\n(a) Solve using Gaussian elimination.\n\n(b) Perturb b_2 by 0.0001 and solve again.\n\n(c) Compute the condition number of A.\n\n(d) Verify that forward error \\approx \\kappa \\times relative perturbation.","type":"content","url":"/exercises-1#q2-12-residuals-and-forward-error","position":35},{"hierarchy":{"lvl1":"Exercises","lvl2":"Computational Exercises"},"type":"lvl2","url":"/exercises-1#computational-exercises","position":36},{"hierarchy":{"lvl1":"Exercises","lvl2":"Computational Exercises"},"content":"","type":"content","url":"/exercises-1#computational-exercises","position":37},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.13: Unstable Recursion","lvl2":"Computational Exercises"},"type":"lvl3","url":"/exercises-1#q2-13-unstable-recursion","position":38},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.13: Unstable Recursion","lvl2":"Computational Exercises"},"content":"Consider evaluating the integralsy_n = \\int_0^1 \\frac{x^n}{10 + x}\\, dx\n\nfor n = 1, 2, \\dots, 30.\n\n(a) Show that y_n + 10y_{n-1} = \\frac{1}{n}.\n\n(b) Show that y_0 = \\log 11 - \\log 10, then use the recursiony_n = \\frac{1}{n} - 10y_{n-1}\n\nto numerically generate y_1 through y_{30}. Be sure to use y_0 in the form above (not \\log(11/10)). Include a table showing n and y_n. Briefly explain what you observe.\n\n(c) Show that for n \\geq 0, we have 0 \\leq y_n \\leq 1. Discuss the results from (b) in light of this bound.\n\n(d) Derive a formula for computing y_{n-1} given y_n (the backward recursion).\n\n(e) Show that y_n equals an infinite series.\n\nHint for (e)\n\nUse the backward recursion y_n = \\frac{1}{10}\\left(\\frac{1}{n+1} - y_{n+1}\\right) assuming y_\\infty = 0. Repeatedly substitute to find the pattern.\n\n(f) Show that for any \\varepsilon > 0 and positive integer n_0, there exists n_1 \\geq n_0 such that taking y_{n_1} = 0 as a starting value produces integral evaluations y_n with absolute error smaller than \\varepsilon for all 0 < n \\leq n_0.\n\n(g) Explain why the backward algorithm is stable.\n\nHint for (g)\n\nInclude a small round-off error at each step. What happens to this error as you iterate backward?\n\n(h) Write a computer program that computes y_{20} within an absolute error of at most \n\n10-5. Explain how you chose n_1.\n\nSolution for (a)\n\nUsing integration by parts or direct manipulation:y_n + 10y_{n-1} = \\int_0^1 \\frac{x^n + 10x^{n-1}}{10+x}\\,dx = \\int_0^1 \\frac{x^{n-1}(x+10)}{10+x}\\,dx = \\int_0^1 x^{n-1}\\,dx = \\frac{1}{n}","type":"content","url":"/exercises-1#q2-13-unstable-recursion","position":39},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.14: Trapezoidal Rule with Round-off","lvl2":"Computational Exercises"},"type":"lvl3","url":"/exercises-1#q2-14-trapezoidal-rule-with-round-off","position":40},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.14: Trapezoidal Rule with Round-off","lvl2":"Computational Exercises"},"content":"Consider computing the integral:I = \\int_1^{2^k} \\frac{dx}{\\sqrt{x}} = 2(2^{k/2} - 1)\n\nusing the trapezoidal rule with k = 22. The approximation is:I \\approx \\Delta x \\left(\\sum_{i=1}^{N-1} f(x_i) + \\frac{f(x_0) + f(x_N)}{2}\\right)\n\nwhere f(x) = x^{-1/2} and \\Delta x = \\frac{2^k - 1}{N}.\n\n(a) Implement this using naive summation with single precision (32-bit) floats. Plot the relative error vs. N for N = 10^p, p \\in [2, 8]. Explain what you observe.\n\n(b) Implement using Kahan summation. Plot the relative error vs. N. Explain what you observe.\n\nHint\n\nTo force single precision in NumPy:s = np.asarray(0.0, dtype=np.float32)","type":"content","url":"/exercises-1#q2-14-trapezoidal-rule-with-round-off","position":41},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.15: Fast Inverse Square Root Accuracy","lvl2":"Computational Exercises"},"type":"lvl3","url":"/exercises-1#q2-15-fast-inverse-square-root-accuracy","position":42},{"hierarchy":{"lvl1":"Exercises","lvl3":"Q2.15: Fast Inverse Square Root Accuracy","lvl2":"Computational Exercises"},"content":"Refer to the fast inverse square root code.\n\n(a) Compute the relative error of the fast inverse square root without Newton iteration (just the bit manipulation). Plot the relative error for inputs x \\in [10^0, 10^5].\n\n(b) Compute the relative error with Newton iterations. How many iterations would you use for a game engine? Support your answer with error plots for 1, 2, and 3 iterations.","type":"content","url":"/exercises-1#q2-15-fast-inverse-square-root-accuracy","position":43},{"hierarchy":{"lvl1":"Fast Inverse Square Root"},"type":"lvl1","url":"/fast-inverse-sqrt","position":0},{"hierarchy":{"lvl1":"Fast Inverse Square Root"},"content":"Big Idea\n\nThe famous fast inverse square root algorithm from Quake III demonstrates how understanding floating point representation enables creative numerical tricks. It combines bit-level manipulation with Newton’s method for refinement.","type":"content","url":"/fast-inverse-sqrt","position":1},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Motivation"},"type":"lvl2","url":"/fast-inverse-sqrt#motivation","position":2},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Motivation"},"content":"Computing unit normal vectors is essential for:\n\nLight reflections in games\n\nCollision detection\n\nComputer-aided design\n\nGiven a vector \\mathbf{v} = (v_1, v_2, v_3), its unit vector is:\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\sqrt{v_1^2 + v_2^2 + v_3^2}}\n\nThis requires evaluating the inverse square root: f(x) = \\frac{1}{\\sqrt{x}}\n\nIn the 1990s when computational power was limited, a fast implementation was crucial for real-time graphics.","type":"content","url":"/fast-inverse-sqrt#motivation","position":3},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"The Famous Code"},"type":"lvl2","url":"/fast-inverse-sqrt#the-famous-code","position":4},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"The Famous Code"},"content":"From the \n\nQuake III Arena source code:float Q_rsqrt( float x )\n{\n    long i;\n    float x2, y;\n    const float threehalfs = 1.5F;\n\n    x2 = x * 0.5F;\n    y  = x;\n    i  = * ( long * ) &y;               // evil floating point bit level hacking\n    i  = 0x5f3759df - ( i >> 1 );       // what the fuck?\n    y  = * ( float * ) &i;\n    y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration\n    return y;\n}\n\nLet’s understand what this does.","type":"content","url":"/fast-inverse-sqrt#the-famous-code","position":5},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"The Key Insight: Logarithms from Bit Patterns"},"type":"lvl2","url":"/fast-inverse-sqrt#the-key-insight-logarithms-from-bit-patterns","position":6},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"The Key Insight: Logarithms from Bit Patterns"},"content":"We want to compute y = \\frac{1}{\\sqrt{x}}. Taking logarithms:\\log_2(y) = -\\frac{1}{2}\\log_2(x)\n\nFrom the IEEE 754 representation of a positive float x:\\text{fl}(x) = 2^{E_x - 127}(1 + m_x)\n\nTaking \\log_2:\\log_2(\\text{fl}(x)) = E_x - 127 + \\log_2(1 + m_x)\n\nSince m_x \\in [0, 1), we can approximate:\\log_2(1 + m_x) \\approx m_x + \\sigma\n\nwhere \\sigma \\approx 0.0430 is a correction constant.","type":"content","url":"/fast-inverse-sqrt#the-key-insight-logarithms-from-bit-patterns","position":7},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"The Magic Formula"},"type":"lvl2","url":"/fast-inverse-sqrt#the-magic-formula","position":8},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"The Magic Formula"},"content":"Substituting m_x = M_x / 2^{23} (where M_x is the integer mantissa):\\log_2(\\text{fl}(x)) \\approx \\frac{1}{2^{23}}(M_x + 2^{23} E_x) + \\sigma - 127\n\nThe key observation: the integer interpretation of float bits approximates the logarithm!\\log_2(\\text{fl}(x)) \\approx \\frac{1}{2^{23}} \\cdot \\text{Int}(x) + \\sigma - 127\n\nApplying this to our inverse square root equation \\log_2(y) = -\\frac{1}{2}\\log_2(x):\\text{Int}(y) = \\underbrace{\\frac{3}{2} \\cdot 2^{23}(127 - \\sigma)}_{\\text{Magic Number}} - \\frac{1}{2}\\text{Int}(x)\n\nThe magic number 0x5f3759df comes from this formula!\n\nThe line i = 0x5f3759df - (i >> 1) computes:\n\ni >> 1: Right shift divides by 2 (the -\\frac{1}{2} factor)\n\nSubtract from magic number: Implements the full formula","type":"content","url":"/fast-inverse-sqrt#the-magic-formula","position":9},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Newton’s Method Refinement"},"type":"lvl2","url":"/fast-inverse-sqrt#newtons-method-refinement","position":10},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Newton’s Method Refinement"},"content":"The bit manipulation gives a rough approximation. To improve accuracy, we apply Newton’s method to:g(y) = \\frac{1}{y^2} - x\n\nThe root of g is y = \\frac{1}{\\sqrt{x}}.\n\nNewton’s iteration:y_{n+1} = y_n - \\frac{g(y_n)}{g'(y_n)} = y_n\\left(\\frac{3}{2} - \\frac{x}{2}y_n^2\\right)\n\nThis is exactly line 12 of the code:y = y * (threehalfs - (x2 * y * y));","type":"content","url":"/fast-inverse-sqrt#newtons-method-refinement","position":11},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Why It Works"},"type":"lvl2","url":"/fast-inverse-sqrt#why-it-works","position":12},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Why It Works"},"content":"Bit manipulation exploits the logarithmic relationship between a float’s value and its bit pattern to get a rough initial guess\n\nNewton’s method rapidly refines this guess (one iteration often suffices for graphics applications)\n\nThe algorithm is remarkably accurate while using only:\n\nOne multiplication\n\nOne subtraction\n\nOne bit shift\n\nOne Newton iteration\n\nNo expensive division or square root operations are needed.","type":"content","url":"/fast-inverse-sqrt#why-it-works","position":13},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Modern Relevance"},"type":"lvl2","url":"/fast-inverse-sqrt#modern-relevance","position":14},{"hierarchy":{"lvl1":"Fast Inverse Square Root","lvl2":"Modern Relevance"},"content":"Modern CPUs have dedicated instructions for inverse square root (e.g., rsqrtss on x86), making this trick less necessary.","type":"content","url":"/fast-inverse-sqrt#modern-relevance","position":15},{"hierarchy":{"lvl1":"Floating Point Numbers"},"type":"lvl1","url":"/floating-point","position":0},{"hierarchy":{"lvl1":"Floating Point Numbers"},"content":"Big Idea\n\nComputers represent real numbers using finite precision floating point arithmetic. Understanding this representation explains why certain computations lose accuracy and helps us write more robust numerical code.","type":"content","url":"/floating-point","position":1},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Number Systems"},"type":"lvl2","url":"/floating-point#number-systems","position":2},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Number Systems"},"content":"","type":"content","url":"/floating-point#number-systems","position":3},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Integers in Base 10","lvl2":"Number Systems"},"type":"lvl3","url":"/floating-point#integers-in-base-10","position":4},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Integers in Base 10","lvl2":"Number Systems"},"content":"We typically write integers in base 10. A number d_N d_{N-1} \\dots d_1 d_0 (where d_i \\in \\{0, 1, \\dots, 9\\}) represents:\\# = d_0 \\cdot 10^0 + d_1 \\cdot 10^1 + \\cdots + d_N \\cdot 10^N","type":"content","url":"/floating-point#integers-in-base-10","position":5},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Binary Representation","lvl2":"Number Systems"},"type":"lvl3","url":"/floating-point#binary-representation","position":6},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Binary Representation","lvl2":"Number Systems"},"content":"Computers use base 2 (binary), where digits d_i \\in \\{0, 1\\}. A binary number d_N d_{N-1} \\dots d_1 d_0 represents:\\# = \\sum_{i=0}^{N} d_i \\cdot 2^i\n\nBinary to Decimal\n\nThe binary number 110_2 = 1 \\cdot 2^2 + 1 \\cdot 2^1 + 0 \\cdot 2^0 = 6.\n\nBit shifting:\n\nLeft shift: 110_2 \\to 1100_2 = 12 (doubles the number)\n\nRight shift: 110_2 \\to 11_2 = 3 (halves the number)","type":"content","url":"/floating-point#binary-representation","position":7},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Signed Integers: Two’s Complement","lvl2":"Number Systems"},"type":"lvl3","url":"/floating-point#signed-integers-twos-complement","position":8},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Signed Integers: Two’s Complement","lvl2":"Number Systems"},"content":"Negative integers use two’s complement representation. For an N-bit signed integer a = d_{N-1}d_{N-2}\\dots d_1 d_0:a = -d_{N-1} \\cdot 2^{N-1} + \\sum_{i=0}^{N-2} d_i \\cdot 2^i\n\nTwo’s Complement Negation\n\nTo get -5 from 5 in 8-bit two’s complement:\n\nStart with 5 = 0000\\,0101_2\n\nInvert all bits: 1111\\,1010_2\n\nAdd one: 1111\\,1011_2 = -5","type":"content","url":"/floating-point#signed-integers-twos-complement","position":9},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Fixed Point Notation"},"type":"lvl2","url":"/floating-point#fixed-point-notation","position":10},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Fixed Point Notation"},"content":"To represent fractions, we allow digits after a radix point:d_N \\dots d_1 d_0 . d_{-1} d_{-2} \\dots d_{-M}\n\nrepresents:\\# = \\sum_{i=-M}^{N} d_i \\cdot b^i\n\nwhere b is the base.\n\nFixed Point Binary\n\nIn binary: 1.01_2 = 1 + 0 \\cdot 2^{-1} + 1 \\cdot 2^{-2} = 1.25\n\nWith finitely many digits, some numbers cannot be represented exactly (e.g., \\frac{1}{3} or \\pi).","type":"content","url":"/floating-point#fixed-point-notation","position":11},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Floating Point Numbers"},"type":"lvl2","url":"/floating-point#floating-point-numbers","position":12},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Floating Point Numbers"},"content":"For scientific computing, we need to represent numbers of vastly different magnitudes—from Avogadro’s number (6.02 \\times 10^{23}) to Planck’s constant (6.63 \\times 10^{-34}).\n\nScientific notation allows the radix point to “float”:245000 = 2.45 \\times 10^5\n\nIn binary:11000_2 = 1.1_2 \\times 2^4, \\quad 0.0101_2 = 1.01_2 \\times 2^{-2}\n\nNote: In normalized binary scientific notation, the digit before the radix point is always 1, so we don’t need to store it!","type":"content","url":"/floating-point#floating-point-numbers","position":13},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"IEEE 754 Standard"},"type":"lvl2","url":"/floating-point#ieee-754-standard","position":14},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"IEEE 754 Standard"},"content":"A floating point number consists of three parts:\n\nSign bit S: 0 for positive, 1 for negative\n\nExponent E: Shifted to allow negative exponents\n\nMantissa/Fraction m: The significant digits","type":"content","url":"/floating-point#ieee-754-standard","position":15},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Single Precision (32-bit)","lvl2":"IEEE 754 Standard"},"type":"lvl3","url":"/floating-point#single-precision-32-bit","position":16},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Single Precision (32-bit)","lvl2":"IEEE 754 Standard"},"content":"Component\n\nBits\n\nSign\n\n1\n\nExponent\n\n8\n\nMantissa\n\n23\n\nThe value represented is:\\text{fl}(x) = \\pm \\left(1 + \\frac{d_0}{2^1} + \\frac{d_1}{2^2} + \\cdots + \\frac{d_{22}}{2^{23}}\\right) \\times 2^{E - 127}\n\nwhere d_i are the mantissa bits and E is the stored exponent.","type":"content","url":"/floating-point#single-precision-32-bit","position":17},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Double Precision (64-bit)","lvl2":"IEEE 754 Standard"},"type":"lvl3","url":"/floating-point#double-precision-64-bit","position":18},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Double Precision (64-bit)","lvl2":"IEEE 754 Standard"},"content":"Component\n\nBits\n\nSign\n\n1\n\nExponent\n\n11\n\nMantissa\n\n52","type":"content","url":"/floating-point#double-precision-64-bit","position":19},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Integer Interpretation of Floating Point"},"type":"lvl2","url":"/floating-point#integer-interpretation-of-floating-point","position":20},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Integer Interpretation of Floating Point"},"content":"The same 32 bits can be interpreted as either a float or an integer. Given the floating point representation (S_x, E_x, m_x), the integer value is:\\text{Int}(x) = 2^{31} S_x + 2^{23} E_x + M_x\n\nwhere M_x = 2^{23} m_x is the integer value of the mantissa bits.\n\nThis dual interpretation is exploited in fast numerical algorithms like the \n\nfast inverse square root.","type":"content","url":"/floating-point#integer-interpretation-of-floating-point","position":21},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Rounding Error (Machine Epsilon)"},"type":"lvl2","url":"/floating-point#rounding-error-machine-epsilon","position":22},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Rounding Error (Machine Epsilon)"},"content":"Given a real number x, its floating point representation \\text{fl}(x) satisfies:\\frac{|\\text{fl}(x) - x|}{|x|} \\leq \\mu\n\nwhere \\mu is the machine epsilon (or unit roundoff):\\mu = \\frac{1}{2} \\times 2^{-t}\n\nwith t being the number of mantissa bits.\n\nPrecision\n\nMantissa bits\n\nMachine epsilon\n\nSingle (float)\n\n23\n\n\\approx 5.96 \\times 10^{-8}\n\nDouble\n\n52\n\n\\approx 1.11 \\times 10^{-16}\n\nPractical Implication\n\nSingle precision gives about 7 decimal digits of accuracy\n\nDouble precision gives about 16 decimal digits of accuracy","type":"content","url":"/floating-point#rounding-error-machine-epsilon","position":23},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Application: The Finite Difference Trade-off"},"type":"lvl2","url":"/floating-point#application-the-finite-difference-trade-off","position":24},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl2":"Application: The Finite Difference Trade-off"},"content":"In the \n\napproximation theory chapter, we observed that finite difference errors increase for very small step sizes. Now we can explain why.","type":"content","url":"/floating-point#application-the-finite-difference-trade-off","position":25},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"The Total Error","lvl2":"Application: The Finite Difference Trade-off"},"type":"lvl3","url":"/floating-point#the-total-error","position":26},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"The Total Error","lvl2":"Application: The Finite Difference Trade-off"},"content":"When computing the forward difference approximation:f'(x) \\approx \\frac{f(x+h) - f(x)}{h}\n\nwe make two types of errors:\n\nTruncation error from Taylor’s theorem: \\frac{h}{2}|f''(\\xi)|\n\nRound-off error from floating-point arithmetic\n\nFor the round-off error: when h is small, f(x+h) \\approx f(x), so we’re subtracting two nearly equal numbers. If both values have relative error \\mu, the subtraction has absolute error roughly 2\\mu|f(x)|. Dividing by h amplifies this to:\\text{Round-off error} \\approx \\frac{2\\mu|f(x)|}{h}\n\nThe total error is:E(h) = \\underbrace{\\frac{2\\mu|f(x)|}{h}}_{\\text{round-off}} + \\underbrace{\\frac{h}{2}|f''(\\xi)|}_{\\text{truncation}}","type":"content","url":"/floating-point#the-total-error","position":27},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"The Optimal Step Size","lvl2":"Application: The Finite Difference Trade-off"},"type":"lvl3","url":"/floating-point#the-optimal-step-size","position":28},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"The Optimal Step Size","lvl2":"Application: The Finite Difference Trade-off"},"content":"To minimize E(h), differentiate and set to zero:\\frac{dE}{dh} = -\\frac{2\\mu|f(x)|}{h^2} + \\frac{|f''(\\xi)|}{2} = 0\n\nSolving (and assuming |f(x)| \\approx |f''(\\xi)| for simplicity):h_{\\text{opt}} = 2\\sqrt{\\mu} \\approx \\sqrt{\\mu}\n\nOptimal Step Sizes\n\nPrecision\n\nMachine epsilon \\mu\n\nOptimal h for FD\n\nSingle\n\n\\sim 10^{-8}\n\n\\sim 10^{-4}\n\nDouble\n\n\\sim 10^{-16}\n\n\\sim 10^{-8}\n\nFor double precision, the optimal forward difference step is h \\approx 10^{-8}.","type":"content","url":"/floating-point#the-optimal-step-size","position":29},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Connection to the Error Framework","lvl2":"Application: The Finite Difference Trade-off"},"type":"lvl3","url":"/floating-point#connection-to-the-error-framework","position":30},{"hierarchy":{"lvl1":"Floating Point Numbers","lvl3":"Connection to the Error Framework","lvl2":"Application: The Finite Difference Trade-off"},"content":"This is a perfect illustration of our error analysis framework:\n\nBackward error (truncation): How much did we perturb the mathematical problem? We approximated f' by a secant slope—error O(h).\n\nForward error (round-off amplification): How much did floating-point errors affect our answer? Error O(\\mu/h).\n\nThe condition number of the operation “subtract then divide by h” grows like 1/h, which is why round-off errors get amplified for small h.\n\nKey Lesson\n\nSmaller step sizes are not always better. There is a sweet spot where truncation and round-off errors balance. This principle appears throughout scientific computing—in finite differences, numerical integration, and iterative solvers.","type":"content","url":"/floating-point#connection-to-the-error-framework","position":31},{"hierarchy":{"lvl1":"Forward and Backward Error"},"type":"lvl1","url":"/forward-backward-error","position":0},{"hierarchy":{"lvl1":"Forward and Backward Error"},"content":"Big Idea\n\nForward error asks: “How wrong is my answer?” Backward error asks: “What problem did I actually solve?” A stable algorithm solves a nearby problem exactly—and the condition number determines whether “nearby problem” means “nearby answer.”","type":"content","url":"/forward-backward-error","position":1},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"A Motivating Example"},"type":"lvl2","url":"/forward-backward-error#a-motivating-example","position":2},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"A Motivating Example"},"content":"Consider computing f(x) = \\sqrt{x+1} - \\sqrt{x} for large x.\n\nThe condition number is \\kappa \\leq 1/2 — this is a well-conditioned problem!\n\nYet using 4-digit decimal arithmetic:f(1984) = \\sqrt{1985} - \\sqrt{1984} = 44.55 - 44.54 = 0.01\n\nThe true value is 0.01122. We lost two significant digits — 10% relative error from a well-conditioned problem!\n\nWhat went wrong? The algorithm subtracts two nearly-equal numbers, amplifying their individual roundoff errors. This is called subtractive cancellation.\n\nA stable alternative: Rationalize the expression:\\sqrt{x+1} - \\sqrt{x} = \\frac{(\\sqrt{x+1} - \\sqrt{x})(\\sqrt{x+1} + \\sqrt{x})}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{1}{89.09} = 0.01122 \\quad \\checkmark\n\nSame problem, different algorithm, full accuracy.\n\nThis example reveals something fundamental: not all errors are the problem’s fault. We need a framework to separate algorithm quality from problem sensitivity.","type":"content","url":"/forward-backward-error#a-motivating-example","position":3},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Forward Error"},"type":"lvl2","url":"/forward-backward-error#forward-error","position":4},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Forward Error"},"content":"Consider computing y = f(x). Due to roundoff or approximation, we actually compute \\tilde{y}.\n\nForward Error\\text{Forward error} = |\\tilde{y} - y| = |\\tilde{y} - f(x)|\n\nQuestion answered: How far is our answer from the truth?\n\nForward error is the most intuitive measure—it directly answers “how wrong is my answer?”","type":"content","url":"/forward-backward-error#forward-error","position":5},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Absolute vs. Relative Error","lvl2":"Forward Error"},"type":"lvl3","url":"/forward-backward-error#absolute-vs-relative-error","position":6},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Absolute vs. Relative Error","lvl2":"Forward Error"},"content":"For meaningful comparisons, we often use relative error:\n\nRelative Forward Error\\text{Relative forward error} = \\frac{|\\tilde{y} - y|}{|y|}\n\nThis measures error as a fraction of the true value.\n\nExample: An error of 0.001 means different things for:\n\ny = 1: relative error is 0.1% (excellent)\n\ny = 0.001: relative error is 100% (useless)\n\nRelative error captures this distinction. When we say “accurate to 6 digits,” we mean relative error \\approx 10^{-6}.\n\nForward Stable Algorithm\n\nAn algorithm is forward stable if:\\frac{|\\tilde{y} - y|}{|y|} = O(\\varepsilon_{\\text{mach}} \\cdot \\kappa)\n\nThe forward error is bounded by machine precision times the condition number.","type":"content","url":"/forward-backward-error#absolute-vs-relative-error","position":7},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"The Catch","lvl2":"Forward Error"},"type":"lvl3","url":"/forward-backward-error#the-catch","position":8},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"The Catch","lvl2":"Forward Error"},"content":"Forward error seems like the natural measure—we want to know how wrong we are! But there’s a problem:\n\nForward error requires knowing the true answer y = f(x).\n\nIf we knew the true answer, we wouldn’t need to compute it. This is where backward error comes in.","type":"content","url":"/forward-backward-error#the-catch","position":9},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Backward Error"},"type":"lvl2","url":"/forward-backward-error#backward-error","position":10},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Backward Error"},"content":"Instead of asking “how wrong is my answer?”, backward error asks a different question.\n\nBackward Error\\text{Backward error} = \\min\\{|\\tilde{x} - x| : f(\\tilde{x}) = \\tilde{y}\\}\n\nQuestion answered: What input would give our output exactly?\n\nIn other words: our computed \\tilde{y} might not equal f(x), but it does equal f(\\tilde{x}) for some nearby \\tilde{x}. The backward error measures how far \\tilde{x} is from x.","type":"content","url":"/forward-backward-error#backward-error","position":11},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Why Backward Error?","lvl2":"Backward Error"},"type":"lvl3","url":"/forward-backward-error#why-backward-error","position":12},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Why Backward Error?","lvl2":"Backward Error"},"content":"At first, backward error seems less useful than forward error. But it turns out to be more fundamental for algorithm analysis. Here’s why:\n\n1. Backward error is computable\n\nForward error requires knowing the true answer—but if we knew that, we wouldn’t need to compute it!\n\nBackward error, on the other hand, is often directly computable:\n\nLinear systems: Given computed \\tilde{x}, the residual r = b - A\\tilde{x} tells us backward error immediately—we solved A\\tilde{x} = b - r exactly.\n\nRoot finding: Given approximate root \\tilde{r}, the residual f(\\tilde{r}) measures backward error.\n\n2. Backward error separates algorithm from problem\n\nThere are two sources of error:\n\nAlgorithm quality: Does the algorithm solve some nearby problem accurately?\n\nProblem sensitivity: How much does the answer change when the problem changes slightly?\n\nBackward error isolates the first question. If an algorithm has small backward error, it’s doing its job well—any remaining forward error is the problem’s fault (ill-conditioning), not the algorithm’s.\n\n3. Backward error accounts for input uncertainty\n\nReal-world inputs have measurement error. If your input x has uncertainty \\delta, then:\n\nYou don’t actually know you’re solving f(x)—you might be solving f(x + \\epsilon) for some |\\epsilon| \\leq \\delta\n\nAn algorithm with backward error \\leq \\delta is as good as exact for your purposes\n\nPrinciple: Don’t ask for more accuracy than your inputs justify.","type":"content","url":"/forward-backward-error#why-backward-error","position":13},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Residual as Backward Error","lvl2":"Backward Error"},"type":"lvl3","url":"/forward-backward-error#residual-as-backward-error","position":14},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Residual as Backward Error","lvl2":"Backward Error"},"content":"For many problems, backward error equals the residual—a directly computable quantity.\n\nProblem\n\nResidual\n\nInterpretation\n\nLinear system Ax = b\n\nr = b - A\\tilde{x}\n\n\\tilde{x} solves Ax = b - r exactly\n\nRoot finding f(x) = 0\n\nf(\\tilde{r})\n\n\\tilde{r} is a root of f(x) - f(\\tilde{r})\n\nLeast squares\n\n|b - A\\tilde{x}|\n\nHow well \\tilde{x} fits the data\n\nBackward Stable Algorithm\n\nAn algorithm is backward stable if for all inputs x, the computed output \\tilde{y} satisfies:\\tilde{y} = f(\\tilde{x}) \\quad \\text{for some } \\tilde{x} \\text{ with } \\frac{|\\tilde{x} - x|}{|x|} = O(\\varepsilon_{\\text{mach}})\n\nThe computed answer is the exact answer to a slightly perturbed problem.\n\nKey insight: Backward stability implies forward stability (for well-conditioned problems). This is why we focus on backward stability—it’s the stronger, more useful guarantee.","type":"content","url":"/forward-backward-error#residual-as-backward-error","position":15},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"The Relationship: Forward Error ≤ κ × Backward Error"},"type":"lvl2","url":"/forward-backward-error#golden-rule","position":16},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"The Relationship: Forward Error ≤ κ × Backward Error"},"content":"Now we connect backward error to forward error using the condition number from the \n\nprevious section.","type":"content","url":"/forward-backward-error#golden-rule","position":17},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Derivation","lvl2":"The Relationship: Forward Error ≤ κ × Backward Error"},"type":"lvl3","url":"/forward-backward-error#derivation","position":18},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"Derivation","lvl2":"The Relationship: Forward Error ≤ κ × Backward Error"},"content":"Suppose we compute \\tilde{y} instead of the true y = f(x). If the algorithm is backward stable:\\tilde{y} = f(\\tilde{x}) \\quad \\text{for some } \\tilde{x} \\text{ near } x\n\nThe forward error is the difference |\\tilde{y} - y| = |f(\\tilde{x}) - f(x)|.\n\nBy Taylor expansion:|f(\\tilde{x}) - f(x)| \\approx |f'(x)| \\cdot |\\tilde{x} - x|\n\nConverting to relative errors:\\frac{|\\tilde{y} - y|}{|y|} \\approx \\frac{|f'(x)|}{|f(x)|} \\cdot |\\tilde{x} - x| = \\underbrace{\\left|\\frac{x f'(x)}{f(x)}\\right|}_{\\kappa} \\cdot \\frac{|\\tilde{x} - x|}{|x|}\n\nThe Golden Rule\\underbrace{\\frac{|\\tilde{y} - y|}{|y|}}_{\\text{relative forward error}} \\lesssim \\underbrace{\\kappa}_{\\text{condition number}} \\cdot \\underbrace{\\frac{|\\tilde{x} - x|}{|x|}}_{\\text{relative backward error}}\n\nForward error ≤ Condition number × Backward error\n\nThis is the central equation of numerical analysis. It cleanly separates:\n\nProblem sensitivity (κ) — intrinsic to the mathematics\n\nAlgorithm quality (backward error) — what we can control","type":"content","url":"/forward-backward-error#derivation","position":19},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"What This Means","lvl2":"The Relationship: Forward Error ≤ κ × Backward Error"},"type":"lvl3","url":"/forward-backward-error#what-this-means","position":20},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl3":"What This Means","lvl2":"The Relationship: Forward Error ≤ κ × Backward Error"},"content":"Component\n\nControlled by\n\nCan we improve it?\n\nForward error\n\nBoth\n\nIndirectly\n\nCondition number \\kappa\n\nThe problem\n\nNo (it’s math)\n\nBackward error\n\nThe algorithm\n\nYes!\n\nPractical implications:\n\nWell-conditioned + backward stable → accurate: Small \\kappa and small backward error guarantee small forward error\n\nIll-conditioned → trouble: Even perfect algorithms give poor forward error when \\kappa is large\n\nFocus on backward stability: Write algorithms with small backward error; that’s all you can control","type":"content","url":"/forward-backward-error#what-this-means","position":21},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Unstable Algorithms: Common Pitfalls"},"type":"lvl2","url":"/forward-backward-error#unstable-algorithms-common-pitfalls","position":22},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Unstable Algorithms: Common Pitfalls"},"content":"Even well-conditioned problems can produce terrible results with unstable algorithms. Three operations are particularly dangerous:\n\nOperation\n\nProblem\n\nStable Alternative\n\nSubtracting close numbers\n\nRelative error explodes\n\nReformulate algebraically\n\nDividing by small numbers\n\nAmplifies numerator error\n\nUse logarithms or rescale\n\nAdding numbers of vastly different magnitude\n\nSmall values lost\n\nSort and sum smallest first\n\nExample: The Quadratic Formula\n\nSolve x^2 - 10^6 x + 1 = 0. The roots are x_1 \\approx 10^6 and x_2 \\approx 10^{-6}.\n\nStandard formula for the small root:x_2 = \\frac{10^6 - \\sqrt{10^{12} - 4}}{2} \\approx \\frac{10^6 - 10^6}{2} = 0\n\nCatastrophic cancellation! The true value is \n\n10-6, so we have 100% relative error.\n\nStable alternative: Use x_1 x_2 = c/a = 1, so x_2 = 1/x_1. Compute x_1 (no cancellation), then x_2 = 1/x_1 with full precision.\n\nExample: Computing e^x - 1\n\nFor small x, direct computation fails:x = 1e-15\nresult = np.exp(x) - 1  # Returns ~1.1e-15 (only 1 correct digit!)\n\nWhy? e^{10^{-15}} \\approx 1.000000000000001, stored as 1.0 due to limited precision. Then 1.0 - 1.0 \\approx 0.\n\nStable alternative: Use the dedicated function:result = np.expm1(1e-15)  # Returns 1.0e-15 (full precision!)\n\nExample: Kahan Summation\n\nWhen summing many numbers, roundoff errors accumulate. Kahan summation tracks and compensates for these errors:def kahan_sum(x):\n    \"\"\"Compensated summation — much more accurate than naive sum.\"\"\"\n    s = 0.0  # Running sum\n    c = 0.0  # Compensation for lost low-order bits\n    for xi in x:\n        y = xi - c        # Compensated value to add\n        t = s + y         # New sum (may lose precision)\n        c = (t - s) - y   # Recover what was lost\n        s = t\n    return s\n\nThis achieves O(\\varepsilon_{\\text{mach}}) error instead of O(n \\cdot \\varepsilon_{\\text{mach}}) for naive summation.","type":"content","url":"/forward-backward-error#unstable-algorithms-common-pitfalls","position":23},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Summary"},"type":"lvl2","url":"/forward-backward-error#summary","position":24},{"hierarchy":{"lvl1":"Forward and Backward Error","lvl2":"Summary"},"content":"Concept\n\nMeasures\n\nControlled by\n\nComputable?\n\nForward error\n\nDistance to true answer\n\nProblem + algorithm\n\nUsually not\n\nBackward error\n\nPerturbation to input\n\nAlgorithm\n\nOften yes (residual)\n\nCondition number\n\nSensitivity of problem\n\nProblem (math)\n\nSometimes\n\nThe workflow:\n\nCheck if the problem is well-conditioned (small \\kappa)\n\nUse a backward-stable algorithm (small backward error)\n\nThen forward error will be small automatically\n\nIf forward error is large despite backward stability, the problem is ill-conditioned—reformulate if possible.","type":"content","url":"/forward-backward-error#summary","position":25},{"hierarchy":{"lvl1":"Error and Stability"},"type":"lvl1","url":"/index-2","position":0},{"hierarchy":{"lvl1":"Error and Stability"},"content":"Big Idea\n\nNumbers on a computer are approximations. Forward error measures how wrong our\nanswer is; backward error measures how much we’d have to perturb the input to\nmake our answer exact. A stable algorithm has small backward error.","type":"content","url":"/index-2","position":1},{"hierarchy":{"lvl1":"Error and Stability","lvl2":"Overview"},"type":"lvl2","url":"/index-2#overview","position":2},{"hierarchy":{"lvl1":"Error and Stability","lvl2":"Overview"},"content":"Every floating-point computation introduces error:\n\nRepresentation error: Most real numbers can’t be stored exactly\n\nRounding error: Arithmetic operations round their results\n\nAccumulation: Errors compound through sequences of operations\n\nUnderstanding these errors—and designing algorithms that control them—is essential for reliable scientific computing.\nThis chapter builds up the error analysis framework in four steps:\n\nFloating-point representation — Computers can’t store most real numbers exactly. Every number has a small representation error bounded by machine epsilon.\n\nThe Quake fast inverse square root — A famous algorithm showing how deep understanding of floating-point enables creative numerical tricks. It bridges to Newton’s method in the next chapter.\n\nCondition numbers — Some mathematical problems amplify errors. The condition number \\kappa measures this sensitivity. This is a property of the problem, not the algorithm.\n\nForward and backward error — Algorithms introduce additional errors. Backward error measures algorithm quality; forward error is what we actually care about.","type":"content","url":"/index-2#overview","position":3},{"hierarchy":{"lvl1":"Error and Stability","lvl2":"The Golden Rule"},"type":"lvl2","url":"/index-2#the-golden-rule","position":4},{"hierarchy":{"lvl1":"Error and Stability","lvl2":"The Golden Rule"},"content":"\\boxed{\\text{Forward error} \\leq \\text{Condition number} \\times \\text{Backward error}}\n\nThis cleanly separates:\n\nProblem sensitivity (\\kappa) — intrinsic to the mathematics, we can’t change it\n\nAlgorithm quality (backward error) — what we can control through careful implementation\n\nA stable algorithm produces answers with small backward error.\nAn ill-conditioned problem amplifies any error, no matter how good the algorithm.","type":"content","url":"/index-2#the-golden-rule","position":5},{"hierarchy":{"lvl1":"Error and Stability","lvl2":"Learning Outcomes"},"type":"lvl2","url":"/index-2#learning-outcomes","position":6},{"hierarchy":{"lvl1":"Error and Stability","lvl2":"Learning Outcomes"},"content":"After completing this chapter, you should be able to:\n\nL2.1: Explain IEEE 754 floating-point representation.\n\nL2.2: Define machine epsilon and its significance.\n\nL2.3: Distinguish forward and backward error.\n\nL2.4: Compute condition numbers for simple functions.\n\nL2.5: Identify sources of numerical instability.\n\nL2.6: Reformulate expressions to avoid cancellation.","type":"content","url":"/index-2#learning-outcomes","position":7},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Instructor"},"type":"lvl2","url":"/#instructor","position":2},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Instructor"},"content":"This is a course developed by Brian van Koten and \n\nAndreas Buttenschoen at the University of Massachusetts Amherst.","type":"content","url":"/#instructor","position":3},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Description"},"type":"lvl2","url":"/#description","position":4},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Description"},"content":"Math 551 is a first course in numerical analysis that introduces students to the\nfundamental concepts and techniques of scientific computing. We will study\nnumerical algorithms, floating-point arithmetic and round-off errors, root-finding\nmethods for nonlinear equations, and numerical linear algebra including direct\nand iterative methods for solving linear systems. The course emphasizes both\ntheoretical understanding and practical implementation using Python.\n\nNote\n\nThese notes are a work in progress. If you find bugs, missing references, typos, or other issues, please \n\nfile an issue on GitHub.","type":"content","url":"/#description","position":5},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Learning Goals"},"type":"lvl2","url":"/#learning-goals","position":6},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Learning Goals"},"content":"Understand floating-point arithmetic and sources of numerical error\n\nApply Taylor’s theorem to analyze the accuracy of numerical approximations\n\nImplement and analyze root-finding algorithms (bisection, Newton’s method, fixed-point iteration)\n\nSolve linear systems using direct methods (LU factorization, Gaussian elimination)\n\nApply iterative methods (Jacobi, Gauss-Seidel) for large sparse systems\n\nConstruct and use polynomial interpolants (Lagrange, Newton, Chebyshev)\n\nPerform matrix computations using Python, NumPy, and SciPy","type":"content","url":"/#learning-goals","position":7},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Prerequisites"},"type":"lvl2","url":"/#prerequisites","position":8},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Prerequisites"},"content":"MATH 233 (Multivariate Calculus)\n\nMATH 235 (Introduction to Linear Algebra)\n\nA scientific programming course: COMPSCI 121, E&C-ENG 122, PHYSICS 281, or E&C-ENG 242","type":"content","url":"/#prerequisites","position":9},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"How to Use This Book"},"type":"lvl2","url":"/#how-to-use-this-book","position":10},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"How to Use This Book"},"content":"Learning Outcomes appear at the start of each chapter. Use them to check your understanding—if you can do what each outcome describes, you’ve mastered that material.\n\nExercises come in three types:\n\nSelf-assessment questions — quick conceptual checks\n\nPencil-and-paper problems — work through by hand to build intuition\n\nComputational exercises — implement algorithms in Python\n\nProblems marked (optional) go beyond the core material.\n\nNotebooks in the \n\nInteractive Notebooks section contain runnable code. Click the Colab badge to open in Google Colaboratory, or use the launch button for Binder/JupyterHub.","type":"content","url":"/#how-to-use-this-book","position":11},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Course Topics"},"type":"lvl2","url":"/#course-topics","position":12},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Course Topics"},"content":"Numerical Algorithms - Taylor polynomials, finite difference approximations, error analysis\n\nRound-off Errors - Floating point arithmetic, machine epsilon, catastrophic cancellation\n\nNonlinear Equations - Bisection method, Newton’s method, fixed point iteration\n\nLinear Algebra Background - Matrix norms, condition numbers, sensitivity analysis\n\nDirect Methods - LU factorization, Gaussian elimination, Cholesky decomposition\n\nIterative Methods - Jacobi method, Gauss-Seidel, convergence analysis\n\nPolynomial Interpolation - Lagrange interpolation, Newton form, Chebyshev polynomials","type":"content","url":"/#course-topics","position":13},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Mathematical Python Resources"},"type":"lvl2","url":"/#mathematical-python-resources","position":14},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Mathematical Python Resources"},"content":"Mathematical Python - Great resource to get started with mathematical Python\n\nNumPy Documentation\n\nSciPy Documentation","type":"content","url":"/#mathematical-python-resources","position":15},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Course Grading Tools"},"type":"lvl2","url":"/#course-grading-tools","position":16},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"Course Grading Tools"},"content":"PLOM Grading will be used to grade quizzes and exams.","type":"content","url":"/#course-grading-tools","position":17},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"License"},"type":"lvl2","url":"/#license","position":18},{"hierarchy":{"lvl1":"MATH 551 Introduction to Scientific Computing","lvl2":"License"},"content":"This work is licensed under a \n\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n","type":"content","url":"/#license","position":19}]}