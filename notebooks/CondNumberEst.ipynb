{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Condition Number Estimation\n",
    "\n",
    "This notebook demonstrates **Hager's algorithm** for estimating the condition number of a matrix, and shows how the error bounds compare to true errors across many random matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as LA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hager-header",
   "metadata": {},
   "source": [
    "## LU Solver with Condition Number Estimation\n",
    "\n",
    "We implement an LU solver class that:\n",
    "1. Uses Hager's algorithm to estimate $\\|A^{-1}\\|$ (and thus $\\kappa(A)$)\n",
    "2. Computes error bounds using the residual\n",
    "3. Optionally applies iterative refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneNorm(x):\n",
    "    return np.sum(np.abs(x))\n",
    "\n",
    "def InfNorm(x):\n",
    "    return np.max(np.abs(x))\n",
    "\n",
    "def ej(x, j):\n",
    "    \"\"\"Unit vector with 1 in position j.\"\"\"\n",
    "    e = np.zeros_like(x)\n",
    "    e[j] = 1.0 \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lu-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LU:\n",
    "    \"\"\"LU factorization with condition number estimation via Hager's algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, A, **kwargs):\n",
    "        self.shape = A.shape\n",
    "        self.op = np.copy(A).astype(np.longdouble)  # Store original for residual computation\n",
    "        \n",
    "        if self.shape[0] != self.shape[1]:\n",
    "            raise RuntimeError(\"Matrix must be square!\")\n",
    "\n",
    "        # Compute LU factorization\n",
    "        self.lu_and_piv = LA.lu_factor(A)\n",
    "\n",
    "    def matmult(self, x, **kwargs):\n",
    "        \"\"\"Multiply by original matrix (or transpose).\"\"\"\n",
    "        transpose = kwargs.get('transpose', False)\n",
    "        if transpose:\n",
    "            return self.op.T @ x\n",
    "        return self.op @ x\n",
    "\n",
    "    def cond(self):\n",
    "        \"\"\"\n",
    "        Estimate ||A^{-1}||_inf using Hager's algorithm.\n",
    "        \n",
    "        Key insight: ||A^{-1}||_inf = ||A^{-T}||_1, so we estimate\n",
    "        the 1-norm of A^{-T} by finding x that maximizes ||A^{-T}x||_1.\n",
    "        \"\"\"\n",
    "        NormBx = 0.0\n",
    "        x = np.ones(self.shape[0], dtype=float) / self.shape[0]\n",
    "      \n",
    "        while True:\n",
    "            # Solve A^T y = x\n",
    "            Bx = self.solve_internal(x, transpose=True)\n",
    "            NewNormBx = OneNorm(Bx)\n",
    "    \n",
    "            if NewNormBx <= NormBx:\n",
    "                break\n",
    "            NormBx = NewNormBx\n",
    "    \n",
    "            # Compute subgradient of 1-norm\n",
    "            chi = np.sign(Bx)\n",
    "            gradF = self.solve_internal(chi)\n",
    "            iMax = np.argmax(np.abs(gradF))\n",
    "    \n",
    "            # Check optimality\n",
    "            if InfNorm(gradF) <= np.dot(gradF, x):\n",
    "                break\n",
    "            \n",
    "            # Update to unit vector at max component\n",
    "            x = ej(x, iMax)\n",
    "\n",
    "        return NormBx\n",
    "\n",
    "    def solve_internal(self, b, **kwargs):\n",
    "        \"\"\"Solve Ax = b (or A^T x = b if transpose=True).\"\"\"\n",
    "        transpose = kwargs.get('transpose', False)\n",
    "        return LA.lu_solve(self.lu_and_piv, b, trans=1 if transpose else 0)\n",
    "\n",
    "    def solve(self, b, **kwargs):\n",
    "        \"\"\"\n",
    "        Solve Ax = b and return solution with error bound.\n",
    "        \n",
    "        Returns:\n",
    "            x: Solution vector\n",
    "            err: Error bound = cond(A) * ||r|| / ||x||\n",
    "        \"\"\"\n",
    "        x = self.solve_internal(b, **kwargs)\n",
    "\n",
    "        # Optional iterative refinement\n",
    "        if kwargs.get('refine', False):\n",
    "            # Compute residual in extended precision\n",
    "            xa = x.astype(np.longdouble)\n",
    "            ba = b.astype(np.longdouble)\n",
    "            r = self.matmult(xa, **kwargs) - ba\n",
    "            # Solve for correction\n",
    "            d = self.solve_internal(np.asarray(r, dtype=float), **kwargs)\n",
    "            x = x - d\n",
    "        \n",
    "        # Compute error bound using residual\n",
    "        r = b - self.matmult(x, **kwargs)\n",
    "        Ainv_norm = self.cond()\n",
    "        err = Ainv_norm * InfNorm(np.asarray(r, dtype=float)) / InfNorm(x)\n",
    "\n",
    "        return x, err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## Quick Test\n",
    "\n",
    "Test on a simple system to verify the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random matrix\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "A = np.random.randn(n, n)\n",
    "x_true = np.ones(n)\n",
    "b = A @ x_true\n",
    "\n",
    "lu = LU(A)\n",
    "x_computed, err_bound = lu.solve(b)\n",
    "\n",
    "true_error = InfNorm(x_computed - x_true) / InfNorm(x_true)\n",
    "\n",
    "print(f\"Estimated ||A^{{-1}}||: {lu.cond():.4e}\")\n",
    "print(f\"True ||A^{{-1}}||:      {np.linalg.norm(np.linalg.inv(A), np.inf):.4e}\")\n",
    "print()\n",
    "print(f\"Error bound:  {err_bound:.4e}\")\n",
    "print(f\"True error:   {true_error:.4e}\")\n",
    "print(f\"Bound / True: {err_bound / true_error:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "study-header",
   "metadata": {},
   "source": [
    "## Error Bound Quality Study\n",
    "\n",
    "How well do our error bounds predict true errors? We test on many random matrices of varying sizes and compare:\n",
    "\n",
    "1. **Standard LU-solve** — no refinement\n",
    "2. **LU-solve with iterative refinement** — one step of refinement using extended precision residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_matrices(M_lower=10, M_upper=500, M_step=50, M_N=10, scale=100.0):\n",
    "    \"\"\"Generate random matrices of various sizes.\"\"\"\n",
    "    matrices = []\n",
    "    for M in range(M_lower, M_upper, M_step):\n",
    "        for _ in range(M_N):\n",
    "            A = scale * (2.0 * np.random.rand(M, M) - 1.0)\n",
    "            matrices.append(A)\n",
    "    return matrices\n",
    "\n",
    "def generate_data(matrices, **lu_kwargs):\n",
    "    \"\"\"Solve systems and collect true errors vs error bounds.\"\"\"\n",
    "    N = len(matrices)\n",
    "    true_errors = np.empty(N)\n",
    "    error_bounds = np.empty(N)\n",
    "    \n",
    "    for i, matrix in enumerate(matrices):\n",
    "        n = matrix.shape[0]\n",
    "        # Use a unit vector as true solution\n",
    "        x_true = ej(np.zeros(n), 1)\n",
    "        b = matrix @ x_true\n",
    "\n",
    "        lu = LU(matrix)\n",
    "        x_computed, err_bound = lu.solve(b, **lu_kwargs)\n",
    "        \n",
    "        true_err = InfNorm(x_computed - x_true) / InfNorm(x_true)\n",
    "        \n",
    "        true_errors[i] = true_err\n",
    "        error_bounds[i] = err_bound\n",
    "\n",
    "    return true_errors, error_bounds\n",
    "\n",
    "# Generate test matrices\n",
    "np.random.seed(123)\n",
    "print(\"Generating random matrices...\")\n",
    "matrices = gen_matrices()\n",
    "print(f\"Generated {len(matrices)} matrices\")\n",
    "\n",
    "# Test different configurations\n",
    "print(\"\\nSolving without refinement...\")\n",
    "true_err_std, bound_std = generate_data(matrices, refine=False)\n",
    "\n",
    "print(\"Solving with refinement...\")\n",
    "true_err_ref, bound_ref = generate_data(matrices, refine=True)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-header",
   "metadata": {},
   "source": [
    "## Results: Error Bounds vs True Errors\n",
    "\n",
    "A good error bound should:\n",
    "- Always be **above** the true error (conservative)\n",
    "- Not be **too far** above (tight)\n",
    "\n",
    "Points on the diagonal line mean the bound equals the true error. The reference lines show bounds that are 10x, 100x, 1000x the true error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-plot",
   "metadata": {},
   "outputs": [],
   "source": "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n\n# Clamp to machine epsilon for log scale\neps = np.finfo(float).eps\ntrue_err_std = np.maximum(eps, true_err_std)\ntrue_err_ref = np.maximum(eps, true_err_ref)\n\n# Plot 1: Standard LU-solve\naxs[0].scatter(true_err_std, bound_std, marker='x', alpha=0.6, s=20)\naxs[0].set_title('Standard LU-solve', fontsize=12)\n\n# Plot 2: With iterative refinement\naxs[1].scatter(true_err_ref, bound_ref, marker='x', alpha=0.6, s=20)\naxs[1].set_title('LU-solve with refinement', fontsize=12)\n\n# Plot 3: Compare true errors (refinement helps?)\naxs[2].scatter(true_err_ref, true_err_std, marker='x', alpha=0.6, s=20)\naxs[2].set_title('True error comparison', fontsize=12)\naxs[2].set_xlabel('True error (with refinement)')\naxs[2].set_ylabel('True error (standard)')\n\n# Add reference lines to first two plots\nfor ax in axs[:2]:\n    xx = np.logspace(-16, -6, 100)\n    ax.plot(xx, xx, 'k-', alpha=0.5, label='bound = true')\n    ax.plot(xx, 10 * xx, 'k--', alpha=0.3, label='10x')\n    ax.plot(xx, 100 * xx, 'k:', alpha=0.3, label='100x')\n    ax.plot(xx, 1000 * xx, 'k-.', alpha=0.3, label='1000x')\n    \n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.set_xlim([1e-16, 1e-8])\n    ax.set_ylim([1e-16, 1e-8])\n    ax.set_xlabel('True Error')\n    ax.set_ylabel('Error Bound')\n    ax.grid(True, alpha=0.3)\n\n# Reference line for comparison plot\nxx = np.logspace(-16, -6, 100)\naxs[2].plot(xx, xx, 'k-', alpha=0.5)\naxs[2].set_xscale('log')\naxs[2].set_yscale('log')\naxs[2].set_xlim([1e-16, 1e-8])\naxs[2].set_ylim([1e-16, 1e-8])\naxs[2].grid(True, alpha=0.3)\n\naxs[0].legend(loc='upper left', fontsize=8)\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "hilbert-header",
   "metadata": {},
   "source": [
    "## Example: Hilbert Matrix\n",
    "\n",
    "The Hilbert matrix is notoriously ill-conditioned. Let's see how our error bounds perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hilbert(n):\n",
    "    a = np.arange(n)\n",
    "    return 1 / (a[:, None] + a[None, :] + 1)\n",
    "\n",
    "print(f\"{'n':>4} | {'cond(H)':>12} | {'Error Bound':>12} | {'True Error':>12} | {'Ratio':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in [5, 8, 10, 12]:\n",
    "    H = hilbert(n)\n",
    "    x_true = np.ones(n)\n",
    "    b = H @ x_true\n",
    "    \n",
    "    cond_H = np.linalg.cond(H, np.inf)\n",
    "    \n",
    "    lu = LU(H)\n",
    "    x_computed, err_bound = lu.solve(b, refine=True)\n",
    "    \n",
    "    true_err = InfNorm(x_computed - x_true) / InfNorm(x_true)\n",
    "    \n",
    "    if true_err > 0:\n",
    "        ratio = err_bound / true_err\n",
    "        print(f\"{n:4d} | {cond_H:12.4e} | {err_bound:12.4e} | {true_err:12.4e} | {ratio:8.1f}x\")\n",
    "    else:\n",
    "        print(f\"{n:4d} | {cond_H:12.4e} | {err_bound:12.4e} | {true_err:12.4e} | exact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "1. **Hager's algorithm** provides a cheap estimate of $\\|A^{-1}\\|$ using only the LU factorization\n",
    "\n",
    "2. **Error bounds** based on $\\kappa(A) \\cdot \\|r\\| / \\|x\\|$ are **conservative** — they overestimate the true error, but usually by a reasonable factor\n",
    "\n",
    "3. **Iterative refinement** can improve accuracy, especially for moderately ill-conditioned systems\n",
    "\n",
    "4. For **very ill-conditioned** matrices (like large Hilbert matrices), even refinement may not help much — the problem itself is nearly singular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}