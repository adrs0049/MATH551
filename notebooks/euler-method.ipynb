{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euler's Method\n",
    "\n",
    "This notebook accompanies the [Forward Euler](../odes/forward-euler.md) lecture notes.\n",
    "We implement forward and backward Euler methods, compare them against exact solutions,\n",
    "and explore convergence, stability, and stiffness interactively."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as LA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d155d60-eb11-44e5-8999-d7db8ee799b7",
   "metadata": {},
   "source": "# Forward Euler\n\nWe solve the scalar test problem $u' = au$, $u(0) = 1$, whose exact solution is $u(t) = e^{at}$. Forward Euler with step size $k$ gives $u_{n+1} = u_n + k\\,f(t_n, u_n)$. The plot compares the numerical and exact solutions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f50af-831e-4984-ab2e-612e9486512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "\n",
    "def f(t, u):\n",
    "    return a * u\n",
    "\n",
    "# Initial condition\n",
    "u0 = 1.0\n",
    "\n",
    "# Time setup\n",
    "Tf = 1.25\n",
    "k  = 1./16  # Time step\n",
    "numsteps = np.ceil(Tf / k).astype(int)\n",
    "k = Tf / numsteps \n",
    "\n",
    "# Create output vector\n",
    "vs = np.empty(numsteps+1, dtype=float)\n",
    "vs[0] = u0\n",
    "\n",
    "v = u0 # This always holds the current ODE value\n",
    "for i in range(numsteps):\n",
    "    t = k * (i - 1)\n",
    "    v = v + k * f(t, v)\n",
    "    vs[i+1] = v\n",
    "\n",
    "# Get timesteps \n",
    "tf = np.linspace(0, Tf, 1000)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(tf, np.exp(a * tf), lw=3, label='Exact solution')\n",
    "\n",
    "# for i in range(1, numsteps+1):\n",
    "#     C = vs[i] * np.exp(-a * ts[i])\n",
    "#     plt.plot(tf, C * np.exp(a * tf), alpha=0.75)\n",
    "\n",
    "ts = np.linspace(0, Tf, numsteps+1)\n",
    "plt.plot(ts, vs, color='k', lw=2, zorder=5, marker='o', label='Euler\\'s methods')\n",
    "\n",
    "plt.title('Euler\\'s method solution k = {0:.4f}'.format(k))\n",
    "plt.ylabel('$y(t)$')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(loc='best')\n",
    "#plt.ylim([0, 100])\n",
    "plt.xlim([0, Tf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff6d64-7a3b-4a0a-8871-9789f7a2d779",
   "metadata": {},
   "source": "# Backward Euler (Secant Method)\n\nBackward Euler is implicit: $u_{n+1} = u_n + k\\,f(t_{n+1}, u_{n+1})$. At each step we must solve a nonlinear equation for $u_{n+1}$. Here we use the **secant method** as the nonlinear solver. We apply it to the same test problem $u' = au$ and compare with the exact solution."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724503cf-bd33-4f72-89c6-28cf61ccd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "\n",
    "def f(t, u):\n",
    "    return a * u\n",
    "\n",
    "# Initial condition\n",
    "u0 = 1.0\n",
    "\n",
    "# Time setup\n",
    "Tf = 1.25\n",
    "k  = 1./16  # Time step\n",
    "numsteps = np.ceil(Tf / k).astype(int)\n",
    "k = Tf / numsteps \n",
    "\n",
    "# Create output vector\n",
    "vs = np.empty(numsteps+1, dtype=float)\n",
    "vs[0] = u0\n",
    "\n",
    "rtol = 1e-4\n",
    "\n",
    "v = u0 # This always holds the current ODE value\n",
    "for i in range(numsteps):\n",
    "    t = k * i  # Time at which we are trying to compute solution at.\n",
    "    \n",
    "    # Setup secant method\n",
    "    v0 = v\n",
    "    v1 = v + k * f(t, v)\n",
    "    \n",
    "    # The function we are trying to find the root of.\n",
    "    def h(v_old, v_new):\n",
    "        return v_old + k * f(t, v_new) - v_new\n",
    "    \n",
    "    # Secant method\n",
    "    done = False\n",
    "    while not done:\n",
    "        v2 = v1 - h(v, v1) * (v1 - v0) / (h(v, v1) - h(v, v0))\n",
    "        v0, v1 = v1, v2\n",
    "        done = abs(v0 - v1) / abs(v0) < rtol\n",
    "\n",
    "    # Update the solution\n",
    "    v = v1\n",
    "    vs[i+1] = v\n",
    "\n",
    "# Get timesteps \n",
    "tf = np.linspace(0, Tf, 1000)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(tf, np.exp(a * tf), lw=3, label='Exact solution')\n",
    "\n",
    "# for i in range(1, numsteps+1):\n",
    "#     C = vs[i] * np.exp(-a * ts[i])\n",
    "#     plt.plot(tf, C * np.exp(a * tf), alpha=0.75)\n",
    "\n",
    "ts = np.linspace(0, Tf, numsteps+1)\n",
    "plt.plot(ts, vs, color='k', lw=2, zorder=5, marker='o', label='Euler\\'s methods')\n",
    "\n",
    "plt.title('Euler\\'s method solution k = {0:.4f}'.format(k))\n",
    "plt.ylabel('$y(t)$')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(loc='best')\n",
    "#plt.ylim([0, 100])\n",
    "plt.xlim([0, Tf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rrt0g4dwijo",
   "source": "## Backward Euler (Simplified Newton)\n\nSame problem, but now using a **simplified Newton** iteration as the nonlinear solver. The Jacobian is approximated by finite differences and held fixed across Newton iterates, which is cheaper per iteration than the secant method above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea542b-d3f4-445f-8c6a-47d1f99a5003",
   "metadata": {},
   "outputs": [],
   "source": "from math import sqrt\n\n# Backward Euler with simplified Newton iteration\na = 2.0\n\ndef f(t, u):\n    return a * u\n\n# Initial condition\nu0 = 1.0\n\n# Time setup\nTf = 1.25\nk  = 1./16  # Time step\nnumsteps = np.ceil(Tf / k).astype(int)\nk = Tf / numsteps \n\n# Create output vector\nvs = np.empty(numsteps+1, dtype=float)\nvs[0] = u0\n\nchi = 1e-1\nrtol = 1e-4\natol = 1e-4\ndelta = sqrt(np.finfo(float).eps)\n\nv = u0 # This always holds the current ODE value\nfor i in range(numsteps):\n    t = k * i  # Time at which we are trying to compute solution at.\n\n    # Get derivative approximation\n    df = (f(t, v + delta) - f(t, v)) / delta\n\n    # Setup simplified Newton\n    v_new = v\n    dv = 1.0\n    done = False\n    while not done:\n        dv_new = (v + k * f(t + k, v_new) - v_new) / (1.0 - k * df)\n        theta = abs(dv_new) / abs(dv)\n        done = dv_new == 0.0 or abs(dv_new) <= chi * atol * (1 - theta) / theta\n        \n        dv = dv_new\n        v_new += dv_new\n\n    # Update the solution\n    v = v_new\n    vs[i+1] = v\n\n# Get timesteps \ntf = np.linspace(0, Tf, 1000)\nfig, ax = plt.subplots(1, 1)\nplt.plot(tf, np.exp(a * tf), lw=3, label='Exact solution')\n\nts = np.linspace(0, Tf, numsteps+1)\nplt.plot(ts, vs, color='k', lw=2, zorder=5, marker='o', label=\"Backward Euler (Newton)\")\n\nplt.title('Backward Euler (simplified Newton) k = {0:.4f}'.format(k))\nplt.ylabel('$y(t)$')\nplt.xlabel('Time')\nplt.legend(loc='best')\nplt.xlim([0, Tf])"
  },
  {
   "cell_type": "markdown",
   "id": "64a4e1db-01ed-4e00-b26c-c2e56cefddc8",
   "metadata": {},
   "source": "# Backward Euler with Richardson Adaptive Step Size\n\nWe add **adaptive step-size control** to backward Euler using Richardson extrapolation.\n\n**Setup.** Starting from $(t, v)$, backward Euler (order $p=1$) is applied in two ways:\n\n1. **One full step** of size $k$ → $v_{\\text{full}}$\n2. **Two half-steps** of size $k/2$ → $v_{\\text{half}}$\n\nBoth approximate $u(t+k)$, but with different leading error terms. For a method of order $p$, the local error after one step of size $h$ is $Ch^{p+1}$:\n\n$$\nv_{\\text{full}} = u(t+k) + Ck^{p+1} + O(k^{p+2})\n$$\n\n$$\nv_{\\text{half}} = u(t+k) + 2C\\left(\\frac{k}{2}\\right)^{p+1} + O(k^{p+2}) = u(t+k) + \\frac{C k^{p+1}}{2^p} + O(k^{p+2})\n$$\n\nThe factor of 2 in the half-step version comes from accumulating error over two steps.\n\n**Error estimate.** Subtracting:\n\n$$\nv_{\\text{half}} - v_{\\text{full}} = Ck^{p+1}\\left(\\frac{1}{2^p} - 1\\right) + O(k^{p+2})\n$$\n\nSo $|v_{\\text{half}} - v_{\\text{full}}|$ estimates the local error, which drives the accept/reject decision and step-size adjustment.\n\n**Extrapolation.** Since we know the error structure, we can cancel the leading error term:\n\n$$\nv_{\\text{Rich}} = v_{\\text{half}} + \\frac{v_{\\text{half}} - v_{\\text{full}}}{2^p - 1}\n$$\n\nFor $p = 1$: $v_{\\text{Rich}} = 2v_{\\text{half}} - v_{\\text{full}}$, which gives an order $p+1 = 2$ accurate solution for free.\n\nRichardson extrapolation does double duty: it provides an **error estimate** for adaptivity and **boosts the solution** from order 1 to order 2."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136fa911-ded4-4885-987b-f6a398089cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from copy import copy\n",
    "\n",
    "def be_adaptive(u0, f, *args, **kwargs):\n",
    "    t0 = kwargs.pop('t0', 0.0)\n",
    "    te = kwargs.pop('te', 1.0)\n",
    "    rtol = kwargs.pop('rtol', 1e-4)\n",
    "    atol = kwargs.pop('atol', 1e-4)\n",
    "    a_min = kwargs.pop('a_min', 0.1)\n",
    "    a_max = kwargs.pop('a_max', 5.0)\n",
    "    safety = kwargs.pop('safety', 0.9)\n",
    "    k_min = kwargs.pop('k_min', 1e-8)\n",
    "    stepmax = kwargs.pop('stepmax', 10000)\n",
    "    n_eqn = u0.size if isinstance(u0, np.ndarray) else 1  # Number of equations\n",
    "    \n",
    "    # Error messages\n",
    "    msg = {0: 'Successful', 2: 'Step size dropped below kmin!', 3: 'Required more than stepmax steps!'}\n",
    "    \n",
    "    # Norm function to use\n",
    "    norm = lambda x: np.max(np.abs(x))\n",
    "    \n",
    "    # Error code\n",
    "    idid = 0\n",
    "    \n",
    "    # Time step setup\n",
    "    k = kwargs.pop('k0', 1e-4 * (te - t0))\n",
    "    \n",
    "    # Current integrator state\n",
    "    v = np.asarray(u0) # This always holds the current ODE value\n",
    "    t = t0             # Current time\n",
    "    \n",
    "    # Logical variable to indicate if we can quit\n",
    "    done = False\n",
    "    \n",
    "    # Some stats\n",
    "    rejected_steps = 0\n",
    "    accepted_steps = 0\n",
    "    \n",
    "    # Storage for output\n",
    "    vs = []\n",
    "    vs.append((t, np.copy(v)))  # Store initial condition\n",
    "    \n",
    "    # Newton method pars\n",
    "    chi = 1e-3\n",
    "    \n",
    "    # Finally integrate\n",
    "    while not done:\n",
    "        if t + k >= te:\n",
    "            k = te - t\n",
    "            done = True\n",
    "        else: # Make sure that the last time step isn't too small.\n",
    "            k = min(k, 0.5 * (te - t))\n",
    "        \n",
    "        # Now compute new steps starting from (t, v)\n",
    "        df = (f(t, v + delta) - f(t, v)) / delta\n",
    "        \n",
    "        # Code Newton method\n",
    "        def newton_iter(v0, df, t, k):\n",
    "            # Setup simplified Newton\n",
    "            v_new = np.copy(v0)\n",
    "            dv = 1.0\n",
    "            done = False\n",
    "            while not done:\n",
    "                dv_new = (v0 + k * f(t, v_new) - v_new) / (1.0 - k * df)\n",
    "                theta = abs(dv_new) / abs(dv)\n",
    "                done = dv_new == 0.0 or abs(dv_new) <= chi * atol * (1 - theta) / theta\n",
    "                dv = dv_new\n",
    "                v_new += dv_new\n",
    "\n",
    "            return v_new\n",
    "        \n",
    "        # Call first Newton iteration\n",
    "        vf_new = newton_iter(v, df, t + k, k)      # Full step\n",
    "        vh_new = newton_iter(v, df, t + 0.5 * k, 0.5 * k) # Half-step\n",
    "        vh_new = newton_iter(vh_new, df, t + k, 0.5 * k) # Half-step\n",
    "        \n",
    "        # Estimate the error\n",
    "        err = max(np.finfo(float).eps, norm(vf_new - vh_new) / (atol + max(norm(vh_new), norm(v)) * rtol))\n",
    "        \n",
    "        # Generate the new time-step\n",
    "        knew = k * min(a_max, max(a_min, safety * sqrt(1./err)))\n",
    "        \n",
    "        # Make sure that new time-step is acceptable\n",
    "        if k < k_min:\n",
    "            idid = 2\n",
    "            break\n",
    "        \n",
    "        if rejected_steps + accepted_steps > stepmax:\n",
    "            idid = 3\n",
    "            break\n",
    "        \n",
    "        if err <= 1.0: # Current error was small enough so update state\n",
    "            # Now do the Richardson extrapolation step\n",
    "            # v_new = vh_new + (vh_new - vf_new) / (2**p - 1)  p = 1 here\n",
    "            v = 2. * vh_new - vf_new\n",
    "            t = t + k\n",
    "            \n",
    "            # Store accepted step\n",
    "            vs.append((t, np.copy(v)))\n",
    "            \n",
    "            # Update stats\n",
    "            accepted_steps += 1\n",
    "        else: # Error was too large -> repeat the step with new step-size\n",
    "            done = False\n",
    "            \n",
    "            # Update stats\n",
    "            rejected_steps += 1\n",
    "        \n",
    "        # Always use new step-size for next step\n",
    "        k = knew\n",
    "\n",
    "    print('Accepted steps = ', accepted_steps, ' Rejected steps = ', rejected_steps)\n",
    "    print('{0:s}'.format(msg[idid]))\n",
    "    return np.array(vs, dtype=[('t', 'float'), ('u', 'f8', (n_eqn, ))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37544d21-7316-401c-a5c2-27ce509e3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -2.0\n",
    "\n",
    "def f(t, u):\n",
    "    return a * u\n",
    "\n",
    "# Initial condition\n",
    "u0 = 1.0\n",
    "\n",
    "data = be_adaptive(u0, f, te=1.2, k0=0.2, atol=1e-3, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995934b5-0021-4fad-9581-c1b36d113207",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = data['t']\n",
    "ue = u0 * np.exp(a * ts)\n",
    "un = data['u'].flatten()\n",
    "aerr = np.max(np.abs(ue - un))\n",
    "rerr = np.max(np.abs((ue - un) / ue))\n",
    "\n",
    "print('aerr = {0:.4e}; rerr = {1:.4e}'.format(aerr, rerr))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].set_title('Number of approximation points = {0:d}'.format(ts.size))\n",
    "#axs[0].plot(ts, np.abs(un - ue), marker='o', label='Numerical', color='k')\n",
    "axs[0].plot(ts, un, label='Numerical', marker='o', color='k', ls='-')\n",
    "axs[0].plot(ts, ue, label='Exact', color='r', ls='--')\n",
    "axs[0].set_xlabel('$t$')\n",
    "axs[0].set_ylabel('$u(t)$')\n",
    "# axs[0].axhline(1e-3, color='r', ls='--')\n",
    "\n",
    "axs[1].set_title('Step sizes')\n",
    "axs[1].plot(ts[:-1], np.diff(ts), marker='o', label='Numerical', color='k')\n",
    "axs[1].set_xlabel('$t$')\n",
    "axs[1].set_ylabel('$k$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ac508-d16f-4f47-a0f9-055dbde3b513",
   "metadata": {},
   "source": "# Backward Euler for Systems\n\nWe extend the adaptive backward Euler solver to **systems of ODEs** ($u \\in \\mathbb{R}^n$). The simplified Newton iteration now requires assembling and LU-factoring the Jacobian matrix $I - kJ_f$. We test on the **van der Pol oscillator**, a classic stiff system where adaptive step sizes are essential."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7392e5b-0b31-4ee8-a292-56f020574984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from copy import copy\n",
    "\n",
    "def ej(j, n):\n",
    "    e = np.zeros(n, dtype=float)\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def be_adaptive(u0, f, *args, **kwargs):\n",
    "    t0 = kwargs.pop('t0', 0.0)\n",
    "    te = kwargs.pop('te', 1.0)\n",
    "    rtol = kwargs.pop('rtol', 1e-4)\n",
    "    atol = kwargs.pop('atol', 1e-4)\n",
    "    a_min = kwargs.pop('a_min', 0.1)\n",
    "    a_max = kwargs.pop('a_max', 2.0)\n",
    "    safety = kwargs.pop('safety', 0.9)\n",
    "    k_min = kwargs.pop('k_min', 1e-8)\n",
    "    stepmax = kwargs.pop('stepmax', 10000)\n",
    "    n_eqn = u0.size if isinstance(u0, np.ndarray) else 1  # Number of equations\n",
    "    \n",
    "    # Error messages\n",
    "    msg = {0: 'Successful', 2: 'Step size dropped below kmin!', 3: 'Required more than stepmax steps!'}\n",
    "    \n",
    "    # Norm function to use\n",
    "    norm = lambda x: np.max(np.abs(x))\n",
    "    \n",
    "    # Error code\n",
    "    idid = 0\n",
    "    \n",
    "    # Time step setup\n",
    "    k = kwargs.pop('k0', 1e-4 * (te - t0))\n",
    "    \n",
    "    # Current integrator state\n",
    "    v = np.asarray(u0) # This always holds the current ODE value\n",
    "    t = t0             # Current time\n",
    "    \n",
    "    # Logical variable to indicate if we can quit\n",
    "    done = False\n",
    "    \n",
    "    # Some stats\n",
    "    rejected_steps = 0\n",
    "    r_newton_steps = 0\n",
    "    accepted_steps = 0\n",
    "    \n",
    "    # Storage for output\n",
    "    vs = []\n",
    "    vs.append((t, np.copy(v)))  # Store initial condition\n",
    "    \n",
    "    # Newton method pars\n",
    "    chi = 1e-1\n",
    "    delta = sqrt(np.finfo(float).eps)\n",
    "    \n",
    "    # Finally integrate\n",
    "    while not done:\n",
    "        if t + k >= te:\n",
    "            k = te - t\n",
    "            done = True\n",
    "        else: # Make sure that the last time step isn't too small.\n",
    "            k = min(k, 0.5 * (te - t))\n",
    "\n",
    "        # Now compute new steps starting from (t, v)\n",
    "        df = np.zeros((n_eqn, n_eqn), dtype=float)\n",
    "        fv = f(t, v)\n",
    "        for i in range(n_eqn):\n",
    "            ei = ej(i, n_eqn)\n",
    "            df[:, i] = (f(t, v + ei * delta) - fv) / delta\n",
    "            \n",
    "        # Now compute LU decomposition of the matrix\n",
    "        # lu, piv = LA.lu_factor(df)\n",
    "        # We need to invert: (I - k A)\n",
    "        # Now solve via:\n",
    "        # x = LA.lu_solve((lu, piv), b)\n",
    "\n",
    "        # Code Newton method\n",
    "        def newton_iter(v0, df, t, k):\n",
    "            # Setup simplified Newton\n",
    "            v_new = np.copy(v0)\n",
    "            done = False\n",
    "\n",
    "            # Assemble the matrix and invert it: Can we do this better?\n",
    "            T = np.eye(n_eqn) - k * df\n",
    "            lu, piv = LA.lu_factor(T)\n",
    "\n",
    "            # Do first iterate\n",
    "            dv_new = LA.lu_solve((lu, piv), (v0 + k * f(t, v_new) - v_new))\n",
    "            dv = dv_new\n",
    "            v_new += dv_new\n",
    "\n",
    "            # Quit if we are done now\n",
    "            done = norm(dv) == 0.0\n",
    "\n",
    "            while not done:\n",
    "                # dv_new = (v0 + k * f(t, v_new) - v_new) / (1.0 - k * df)\n",
    "                dv_new = LA.lu_solve((lu, piv), (v0 + k * f(t, v_new) - v_new))\n",
    "                theta = norm(dv_new) / norm(dv)\n",
    "                # print('theta = ', theta, ' error tol = ', chi * atol * (1 - theta) / theta)\n",
    "                if theta > 1.0:\n",
    "                    break\n",
    "\n",
    "                done = norm(dv_new) == 0.0 or norm(dv_new) <= chi * atol * (1 - theta) / theta\n",
    "                dv = dv_new\n",
    "                v_new += dv_new\n",
    "\n",
    "            return v_new, theta\n",
    "        \n",
    "        # Call first Newton iteration\n",
    "        vf_new, theta = newton_iter(v,      df, t + k, k)      # Full step\n",
    "        if theta > 1.:\n",
    "            r_newton_steps += 1\n",
    "            k = k * a_min\n",
    "            continue\n",
    "        \n",
    "        vh_new, theta = newton_iter(v,      df, t + 0.5 * k, 0.5 * k) # Half-step\n",
    "        if theta > 1.:\n",
    "            r_newton_steps += 1\n",
    "            k = k * a_min\n",
    "            continue\n",
    "            \n",
    "        vh_new, theta = newton_iter(vh_new, df, t + k, 0.5 * k) # Half-step\n",
    "        if theta > 1.:\n",
    "            r_newton_steps += 1\n",
    "            k = k * a_min\n",
    "            continue\n",
    "        \n",
    "        # Estimate the error\n",
    "        err = max(np.finfo(float).eps, norm(vf_new - vh_new) / (atol + max(norm(vh_new), norm(v)) * rtol))\n",
    "        \n",
    "        # Generate the new time-step\n",
    "        knew = k * min(a_max, max(a_min, safety * sqrt(1./err)))\n",
    "        \n",
    "        # Make sure that new time-step is acceptable\n",
    "        if k < k_min:\n",
    "            idid = 2\n",
    "            break\n",
    "        \n",
    "        if rejected_steps + accepted_steps > stepmax:\n",
    "            idid = 3\n",
    "            break\n",
    "        \n",
    "        if err <= 1.0: # Current error was small enough so update state\n",
    "            # Now do the Richardson extrapolation step\n",
    "            # v_new = vh_new + (vh_new - vf_new) / (2**p - 1)  p = 1 here\n",
    "            v = 2. * vh_new - vf_new\n",
    "            t = t + k\n",
    "            \n",
    "            # Store accepted step\n",
    "            vs.append((t, np.copy(v)))\n",
    "            \n",
    "            # Update stats\n",
    "            accepted_steps += 1\n",
    "        else: # Error was too large -> repeat the step with new step-size\n",
    "            done = False\n",
    "            \n",
    "            # Update stats\n",
    "            rejected_steps += 1\n",
    "        \n",
    "        # Always use new step-size for next step\n",
    "        k = knew\n",
    "\n",
    "    print('Accepted steps = ', accepted_steps, \n",
    "          ' Rejected steps = ', rejected_steps,\n",
    "          ' Rejected Newton steps = ', r_newton_steps)\n",
    "    print('{0:s}'.format(msg[idid]))\n",
    "    return np.array(vs, dtype=[('t', 'float'), ('u', 'f8', (n_eqn, ))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da9bf6-a231-490b-9055-584a71408bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-2\n",
    "\n",
    "def f(t, y):\n",
    "    return np.array([\n",
    "        y[1],\n",
    "        ((1. - y[0]**2) * y[1] - y[0]) / eps\n",
    "    ])\n",
    "\n",
    "ic = np.array([2.0, 0.])\n",
    "\n",
    "t0 = 0.0\n",
    "te = 5.0\n",
    "\n",
    "data1 = be_adaptive(ic, f, te=te, stepmax=400000, k0=0.1, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b68f2-739f-488a-b062-927572253a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(6.5, 5), sharex=True)\n",
    "\n",
    "ts = data1['t']\n",
    "un = data1['u']\n",
    "\n",
    "ks = np.diff(ts)\n",
    "ks = np.hstack((ks, ks[-1]))\n",
    "\n",
    "axs[0].plot(ts, un[:, 0])\n",
    "axs[0].set_ylabel('$u_1(t)$')\n",
    "axs[1].plot(ts, un[:, 1])\n",
    "axs[1].set_ylabel('$u_2(t)$')\n",
    "axs[2].semilogy(ts, ks)\n",
    "axs[2].set_ylabel('$k$')\n",
    "axs[2].set_xlabel('Time $t$')\n",
    "# axs[2].set_ylim([1e-7, 5e-3])\n",
    "# axs[2].axhline(0.33 * eps, color='r', ls='--')\n",
    "for ax in axs:\n",
    "    ax.set_xlim([t0, te])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a6624-031e-42ce-ab12-9915cafb01b2",
   "metadata": {},
   "source": "# Comparing Methods: Euler, RK2, Adams-Bashforth, PECE\n\nWe implement several one-step and multistep methods on the same test problem $u' = au$ and compare accuracy at fixed step size. Methods shown: forward Euler (order 1), RK2 midpoint (order 2), Adams-Bashforth 2-step (order 2), and predictor-corrector (PECE) variants. The plot overlays all methods against the exact solution."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07644d0-fcb0-45e5-9063-4c0f31cef434",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "\n",
    "def f(t, u):\n",
    "    return a * u\n",
    "\n",
    "# Initial condition\n",
    "u0 = 1.0\n",
    "\n",
    "# Time setup\n",
    "Tf = 1.25\n",
    "k  = 1./8  # Time step\n",
    "numsteps = np.ceil(Tf / k).astype(int)\n",
    "k = Tf / numsteps \n",
    "ts = np.arange(numsteps + 1) * k\n",
    "ue = u0 * np.exp(a * ts)\n",
    "\n",
    "# Create output vector\n",
    "def fwd_euler(u0, numsteps):\n",
    "    vs = np.empty(numsteps+1, dtype=float)\n",
    "    vs[0] = u0\n",
    "    \n",
    "    v = u0 # This always holds the current ODE value\n",
    "    for i in range(numsteps):\n",
    "        t = k * (i - 1)\n",
    "        v = v + k * f(t, v)\n",
    "        vs[i+1] = v\n",
    "\n",
    "    return vs\n",
    "\n",
    "def rk2(u0, numsteps):\n",
    "    vs = np.empty(numsteps+1, dtype=float)\n",
    "    vs[0] = u0\n",
    "    \n",
    "    # Method constants\n",
    "    a = 0.0 \n",
    "    b = 1.0 \n",
    "    alpha = 0.5 \n",
    "    beta  = 0.5\n",
    "    \n",
    "    v = u0 # This always holds the current ODE value\n",
    "    for i in range(numsteps):\n",
    "        t  = k * (i - 1)\n",
    "        r1 = f(t, v)\n",
    "        r2 = f(t + alpha * k, v + beta * k * r1)\n",
    "        v = v + k * (a * r1 + b * r2)\n",
    "        vs[i+1] = v\n",
    "\n",
    "    return vs\n",
    "\n",
    "def adams_bashford(u0, numsteps):\n",
    "    vs = np.empty(numsteps+1, dtype=float)\n",
    "    vs[0] = u0\n",
    "\n",
    "    # 1. Do one step of Euler\n",
    "    vs[1] = u0 + k * f(0, u0)\n",
    "\n",
    "    for i in range(1, numsteps):\n",
    "        vs[i+1] = vs[i] + 1.5 * k * f(i * k, vs[i]) - 0.5 * k * f((i - 1) * k, vs[i-1])\n",
    "\n",
    "    return vs\n",
    "\n",
    "\n",
    "# Create output vector\n",
    "def pece(u0, numpsteps):\n",
    "    vs = np.empty(numsteps+1, dtype=float)\n",
    "    vs[0] = u0\n",
    "    \n",
    "    v = u0 # This always holds the current ODE value\n",
    "    for i in range(numsteps):\n",
    "        t = k * (i - 1)\n",
    "        # 1. Predictor\n",
    "        ftv = f(t, v)\n",
    "        y_star = v + k * ftv\n",
    "\n",
    "        # 2. Correctors\n",
    "        t_new = k * i\n",
    "        v = v + 0.5 * k * (ftv + f(t_new, y_star))\n",
    "\n",
    "        # 3. Store result\n",
    "        vs[i+1] = v\n",
    "        \n",
    "    return vs\n",
    "\n",
    "\n",
    "def adams_pece(u0, numsteps):\n",
    "    vs = np.empty(numsteps+1, dtype=float)\n",
    "    vs[0] = u0\n",
    "\n",
    "    # 1. Do one step of PECE Euler\n",
    "    y_star = u0 + k * f(0, u0)\n",
    "    vs[1] = vs[0] + 0.5 * k * (f(0, u0) + f(k, y_star))\n",
    "\n",
    "    for i in range(1, numsteps):\n",
    "        y_star = vs[i] + 1.5 * k * f(i * k, vs[i]) - 0.5 * k * f((i - 1) * k, vs[i-1])\n",
    "        vs[i+1] = vs[i] + 0.5 * k * (f((i + 1)*k, y_star) + f(i * k, vs[i]))\n",
    "\n",
    "    return vs\n",
    "\n",
    "\n",
    "# run some sims\n",
    "v1s = fwd_euler(u0, numsteps)\n",
    "e1s = np.max(np.abs(v1s - ue))\n",
    "\n",
    "v5s = rk2(u0, numsteps)\n",
    "e5s = np.max(np.abs(v5s - ue))\n",
    "\n",
    "v2s = pece(u0, numsteps)\n",
    "e2s = np.max(np.abs(v2s - ue))\n",
    "\n",
    "v3s = adams_bashford(u0, numsteps)\n",
    "e3s = np.max(np.abs(v3s - ue))\n",
    "\n",
    "v4s = adams_pece(u0, numsteps)\n",
    "e4s = np.max(np.abs(v4s - ue))\n",
    "\n",
    "# Get timesteps \n",
    "tf = np.linspace(0, Tf, 1000)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(tf, np.exp(a * tf), lw=4, label='Exact solution')\n",
    "\n",
    "# for i in range(1, numsteps+1):\n",
    "#     C = vs[i] * np.exp(-a * ts[i])\n",
    "#     plt.plot(tf, C * np.exp(a * tf), alpha=0.75)\n",
    "\n",
    "ts = np.linspace(0, Tf, numsteps+1)\n",
    "plt.plot(ts, v1s, color='k', lw=2, ls=':', zorder=4, marker='o', label='Forward Euler')\n",
    "plt.plot(ts, v5s, color='k', lw=2, ls='-.', zorder=4, marker='o', label='RK2')\n",
    "# plt.plot(ts, v2s, color='r', lw=2, zorder=5, marker='o', label='PECE Euler')\n",
    "# plt.plot(ts, v3s, color='b', lw=2, ls=':', zorder=4, marker='o', label='Adams')\n",
    "# plt.plot(ts, v4s, color='b', lw=2, zorder=5, marker='o', label='PECE Adams')\n",
    "\n",
    "plt.title('Euler\\'s method solution k = {0:.4f}'.format(k))\n",
    "plt.ylabel('$y(t)$')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(loc='best')\n",
    "#plt.ylim([0, 100])\n",
    "plt.xlim([0, Tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574bb1a-22b3-4b9e-b64e-63f6846dc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Euler E1 = %.4g' % e1s)\n",
    "print('PECE Euler E2 = %.4g' % e2s)\n",
    "print('Adams E3 = %.4g' % e3s)\n",
    "print('PECE Adams E4 = %.4g' % e4s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299540cc-972a-4439-9b85-059091fd2463",
   "metadata": {},
   "source": "# Adaptive Step-Size Control with RK2\n\nWe implement an adaptive RK2 (Heun) integrator using the **embedded pair** idea: the difference between a forward Euler step and an RK2 step estimates the local truncation error. The step size is adjusted so the estimated error stays below a user-specified tolerance. The plots show the solution and the adapted step sizes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2a685-cfac-44e5-a1b6-a97a338d4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def rk2_adaptive(u0, f, *args, **kwargs):\n",
    "    t0 = kwargs.pop('t0', 0.0)\n",
    "    te = kwargs.pop('te', 1.0)\n",
    "    rtol = kwargs.pop('rtol', 1e-4)\n",
    "    atol = kwargs.pop('atol', 1e-4)\n",
    "    a_min = kwargs.pop('a_min', 0.1)\n",
    "    a_max = kwargs.pop('a_max', 2.0)\n",
    "    safety = kwargs.pop('safety', 0.9)\n",
    "    k_min = kwargs.pop('k_min', 1e-8)\n",
    "    stepmax = kwargs.pop('stepmax', 10000)\n",
    "    n_eqn = u0.size if isinstance(u0, np.ndarray) else 1  # Number of equations\n",
    "    \n",
    "    # Error messages\n",
    "    msg = {0: 'Success!', 2: 'Step size dropped below kmin!', 3: 'Required more than stepmax steps!'}\n",
    "    \n",
    "    # Error code\n",
    "    idid = 0\n",
    "    \n",
    "    # Norm function to use\n",
    "    norm = lambda x: np.max(np.abs(x))\n",
    "    \n",
    "    # Time step setup\n",
    "    k = kwargs.pop('k0', 1e-4 * (te - t0))\n",
    "    \n",
    "    # Constants for the RK2 method\n",
    "    a = 0.5\n",
    "    b = 0.5\n",
    "    alpha = 1.0 \n",
    "    beta  = 1.0\n",
    "    \n",
    "    # Current integrator state\n",
    "    v = np.asarray(u0) # This always holds the current ODE value\n",
    "    t = t0             # Current time\n",
    "    \n",
    "    # Logical variable to indicate if we can quit\n",
    "    done = False\n",
    "    \n",
    "    # Some stats\n",
    "    rejected_steps = 0\n",
    "    accepted_steps = 0\n",
    "    \n",
    "    # Storage for output\n",
    "    vs = []\n",
    "    vs.append((t, v))  # Store initial condition\n",
    "    \n",
    "    # Finally integrate\n",
    "    while not done:\n",
    "        if t + k >= te:\n",
    "            k = te - t\n",
    "            done = True\n",
    "        else: # Make sure that the last time step isn't too small.\n",
    "            k = min(k, 0.5 * (te - t))\n",
    "        \n",
    "        # Now compute new steps starting from (t, v)\n",
    "        # RK2: Compute the two slopes\n",
    "        r1 = f(t, v)\n",
    "        r2 = f(t + alpha * k, v + beta * k * r1)\n",
    "        \n",
    "        # Compute Euler step\n",
    "        w = v + k * r1\n",
    "        \n",
    "        # Compute RK2 step\n",
    "        vnew = v + k * (a * r1 + b * r2)\n",
    "        \n",
    "        # Estimate the error\n",
    "        err = max(np.finfo(float).eps, norm(w - vnew) / (atol + max(norm(vnew), norm(v)) * rtol))\n",
    "        \n",
    "        # Generate the new time-step\n",
    "        knew = k * min(a_max, max(a_min, safety * sqrt(1./err)))\n",
    "        \n",
    "        # Make sure that new time-step is acceptable\n",
    "        if k < k_min:\n",
    "            idid = 2\n",
    "            break\n",
    "            \n",
    "        if accepted_steps + rejected_steps > stepmax:\n",
    "            idid = 3\n",
    "            break\n",
    "        \n",
    "        if err <= 1.0: # Current error was small enough so update state\n",
    "            v = vnew\n",
    "            t = t + k\n",
    "            \n",
    "            # Store accepted step\n",
    "            vs.append((t, v))\n",
    "            \n",
    "            # Update stats\n",
    "            accepted_steps += 1\n",
    "        else: # Error was too large -> repeat the step with new step-size\n",
    "            done = False\n",
    "            \n",
    "            # Update stats\n",
    "            rejected_steps += 1\n",
    "        \n",
    "        # Always use new step-size for next step\n",
    "        k = knew\n",
    "\n",
    "    print('{0:s}'.format(msg[idid]))\n",
    "    print('Accepted steps = ', accepted_steps, ' Rejected steps = ', rejected_steps)\n",
    "    return np.array(vs, dtype=[('t', 'float'), ('u', 'f8', (n_eqn, ))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c578f4f-9f9f-4a61-81a5-3ea51e5ae2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "\n",
    "def f(t, u):\n",
    "    return a * u\n",
    "\n",
    "# Initial condition\n",
    "u0 = 1.0\n",
    "\n",
    "te = 1.15\n",
    "data = rk2_adaptive(u0, f, te=te, k0=0.1, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98ce64-a72b-4e7b-bbc9-68215139ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = data['t']\n",
    "ue = u0 * np.exp(a * ts)\n",
    "un = data['u'].flatten()\n",
    "aerr = np.max(np.abs(ue - un))\n",
    "rerr = np.max(np.abs((ue - un) / ue))\n",
    "\n",
    "print('aerr = {0:.4e}; rerr = {1:.4e}'.format(aerr, rerr))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].set_title('Number of approximation points = {0:d}'.format(ts.size))\n",
    "axs[0].plot(ts, un, marker='o', label='Numerical', color='k')\n",
    "axs[0].plot(ts, ue, label='Exact', color='r', ls='--')\n",
    "axs[0].set_xlabel('$t$')\n",
    "axs[0].set_ylabel('$u(t)$')\n",
    "axs[0].axvline(te, color='r', ls='--')\n",
    "\n",
    "axs[1].set_title('Step sizes')\n",
    "axs[1].plot(ts[:-1], np.diff(ts), marker='o', label='Numerical', color='k')\n",
    "axs[1].set_xlabel('$t$')\n",
    "axs[1].set_ylabel('$k$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cece366-1ce9-4bea-821e-a33197a5c48f",
   "metadata": {},
   "source": "# Adaptive Predictor-Corrector\n\nAn adaptive **PECE** (Predict-Evaluate-Correct-Evaluate) integrator: the predictor is a forward Euler step, the corrector applies the trapezoidal rule. Their difference estimates the local error and drives step-size control. Tested on $u' = au$ with decaying solution."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16072769-c490-4749-8bc7-4008a5cc71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def pc_adaptive(u0, f, *args, **kwargs):\n",
    "    t0 = kwargs.pop('t0', 0.0)\n",
    "    te = kwargs.pop('te', 1.0)\n",
    "    rtol = kwargs.pop('rtol', 1e-4)\n",
    "    atol = kwargs.pop('atol', 1e-4)\n",
    "    a_min = kwargs.pop('a_min', 0.1)\n",
    "    a_max = kwargs.pop('a_max', 5.0)\n",
    "    safety = kwargs.pop('safety', 0.9)\n",
    "    k_min = kwargs.pop('k_min', 1e-8)\n",
    "    stepmax = kwargs.pop('stepmax', 10000)\n",
    "    n_eqn = u0.size if isinstance(u0, np.ndarray) else 1  # Number of equations\n",
    "    \n",
    "    # Error messages\n",
    "    msg = {0: 'Successful', 2: 'Step size dropped below kmin!', 3: 'Required more than stepmax steps!'}\n",
    "    \n",
    "    # Norm function to use\n",
    "    norm = lambda x: np.max(np.abs(x))\n",
    "    \n",
    "    # Error code\n",
    "    idid = 0\n",
    "    \n",
    "    # Time step setup\n",
    "    k = kwargs.pop('k0', 1e-4 * (te - t0))\n",
    "    \n",
    "    # Current integrator state\n",
    "    v = np.asarray(u0) # This always holds the current ODE value\n",
    "    t = t0             # Current time\n",
    "    \n",
    "    # Logical variable to indicate if we can quit\n",
    "    done = False\n",
    "    \n",
    "    # Some stats\n",
    "    rejected_steps = 0\n",
    "    accepted_steps = 0\n",
    "    \n",
    "    # Storage for output\n",
    "    vs = []\n",
    "    vs.append((t, v))  # Store initial condition\n",
    "    \n",
    "    # Finally integrate\n",
    "    while not done:\n",
    "        if t + k >= te:\n",
    "            k = te - t\n",
    "            done = True\n",
    "        else: # Make sure that the last time step isn't too small.\n",
    "            k = min(k, 0.5 * (te - t))\n",
    "        \n",
    "        # Now compute new steps starting from (t, v)\n",
    "        # RK2: Compute the two slopes\n",
    "        r1 = f(t, v)\n",
    "        \n",
    "        # Compute Euler step i.e. the predictors\n",
    "        w = v + k * r1\n",
    "        \n",
    "        # Apply the corrector i.e. trapezoidal rule\n",
    "        vnew = v + 0.5 * k * (r1 + f(t + k, w))\n",
    "        \n",
    "        # Estimate the error\n",
    "        err = max(np.finfo(float).eps, norm(w - vnew) / (atol + max(norm(vnew), norm(v)) * rtol))\n",
    "        \n",
    "        # Generate the new time-step\n",
    "        knew = k * min(a_max, max(a_min, safety * sqrt(1./err)))\n",
    "        \n",
    "        # Make sure that new time-step is acceptable\n",
    "        if k < k_min:\n",
    "            idid = 2\n",
    "            break\n",
    "        \n",
    "        if rejected_steps + accepted_steps > stepmax:\n",
    "            idid = 3\n",
    "            break\n",
    "        \n",
    "        if err <= 1.0: # Current error was small enough so update state\n",
    "            v = vnew\n",
    "            t = t + k\n",
    "            \n",
    "            # Store accepted step\n",
    "            vs.append((t, v))\n",
    "            \n",
    "            # Update stats\n",
    "            accepted_steps += 1\n",
    "        else: # Error was too large -> repeat the step with new step-size\n",
    "            done = False\n",
    "            \n",
    "            # Update stats\n",
    "            rejected_steps += 1\n",
    "        \n",
    "        # Always use new step-size for next step\n",
    "        k = knew\n",
    "\n",
    "    print('Accepted steps = ', accepted_steps, ' Rejected steps = ', rejected_steps)\n",
    "    print('{0:s}'.format(msg[idid]))\n",
    "    return np.array(vs, dtype=[('t', 'float'), ('u', 'f8', (n_eqn, ))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c01602-109f-4f85-a9f9-3623843e835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -2.0\n",
    "\n",
    "def f(t, u):\n",
    "    return a * u\n",
    "\n",
    "# Initial condition\n",
    "u0 = 1.0\n",
    "\n",
    "data = pc_adaptive(u0, f, te=1.2, k0=0.1, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315c72d-3de2-4d4b-81a5-b267ec84e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = data['t']\n",
    "ue = u0 * np.exp(a * ts)\n",
    "un = data['u'].flatten()\n",
    "aerr = np.max(np.abs(ue - un))\n",
    "rerr = np.max(np.abs((ue - un) / ue))\n",
    "\n",
    "print('aerr = {0:.4e}; rerr = {1:.4e}'.format(aerr, rerr))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].set_title('Number of approximation points = {0:d}'.format(ts.size))\n",
    "#axs[0].plot(ts, un, marker='o', label='Numerical', color='k')\n",
    "axs[0].plot(ts, np.abs(un - ue), marker='o', label='Numerical', color='k')\n",
    "#axs[0].plot(ts, ue, label='Exact', color='r', ls='--')\n",
    "axs[0].axhline(1e-2, color='r', ls='--')\n",
    "axs[0].set_xlabel('$t$')\n",
    "axs[0].set_ylabel('$u(t)$')\n",
    "\n",
    "axs[1].set_title('Step sizes')\n",
    "axs[1].plot(ts[:-1], np.diff(ts), marker='o', label='Numerical', color='k')\n",
    "axs[1].set_xlabel('$t$')\n",
    "axs[1].set_ylabel('$k$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba43145f-6e3b-4f21-a084-25a7ab6c2b4d",
   "metadata": {},
   "source": "# Van der Pol Oscillator (Stiff System)\n\nThe van der Pol oscillator with small $\\varepsilon$ is a classic **stiff** problem: the solution has sharp transients separated by slow phases. The adaptive RK2 integrator must take very small steps during transients and can relax during slow phases. The bottom panel shows how the step size adapts automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb54d2-9f86-46e2-93bc-814b22a04e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-2\n",
    "\n",
    "def f(t, y):\n",
    "    return np.array([\n",
    "        y[1],\n",
    "        ((1. - y[0]**2) * y[1] - y[0]) / eps\n",
    "    ])\n",
    "    \n",
    "\n",
    "ic = np.array([2.0, 0.])\n",
    "\n",
    "t0 = 0.0\n",
    "te = 5.0\n",
    "\n",
    "data1 = rk2_adaptive(ic, f, te=te, stepmax=400000, k0=0.1, atol=1e-2, rtol=1e-2)\n",
    "# data2 = pc_adaptive(ic, f, te=te, stepmax=40000, k0=0.1, atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149e64d-88ad-4159-8b13-e96a730770d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(6.5, 5), sharex=True)\n",
    "\n",
    "ts = data1['t']\n",
    "un = data1['u']\n",
    "\n",
    "ks = np.diff(ts)\n",
    "ks = np.hstack((ks, ks[-1]))\n",
    "\n",
    "axs[0].plot(ts, un[:, 0])\n",
    "axs[0].set_ylabel('$u_1(t)$')\n",
    "axs[1].plot(ts, un[:, 1])\n",
    "axs[1].set_ylabel('$u_2(t)$')\n",
    "axs[2].semilogy(ts, ks)\n",
    "axs[2].set_ylabel('$k$')\n",
    "axs[2].set_xlabel('Time $t$')\n",
    "# axs[2].set_ylim([1e-7, 5e-3])\n",
    "# axs[2].axhline(0.33 * eps, color='r', ls='--')\n",
    "for ax in axs:\n",
    "    ax.set_xlim([t0, te])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443002e-af51-40bc-86f4-d5cc6c5aa570",
   "metadata": {},
   "source": "# Stability of Forward Euler\n\nWe solve $u' = \\lambda(u - \\cos t) - \\sin t$, which has exact solution $u(t) = \\cos t$, for three values of $\\lambda$. With fixed step size $h = 10^{-3}$: $\\lambda = 0$ and $\\lambda = -10$ are stable, but $\\lambda = -2100$ violates the stability condition $h < 2/|\\lambda| \\approx 9.5 \\times 10^{-4}$ and the solution blows up."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca0a18-f06d-4ed6-8db4-ed6739492706",
   "metadata": {},
   "outputs": [],
   "source": "def f1(t, u):\n    return -np.sin(t)\n\n# setup problem\nu0 = 1.0\nt0 = 0.0\nte = 2.0\n\n# Create output vector\ndef fwd_euler(u0, f, k, numsteps):\n    vs = np.empty(numsteps+1, dtype=float)\n    vs[0] = u0\n    \n    v = u0 # This always holds the current ODE value\n    for i in range(numsteps):\n        t = k * i\n        v = v + k * f(t, v)\n        vs[i+1] = v\n\n    return vs\n\n# First goal with k = 1e-3\nk  = 1e-3\nnumsteps = np.ceil(te / k).astype(int)\nk = te / numsteps \nts = np.arange(numsteps + 1) * k\nue = np.cos(ts)\n\n# Execute Euler's method\nvs1 = fwd_euler(u0, f1, k, numsteps)\nerr = np.abs(vs1 - ue)[-1]  # Grab last error\n\nprint(f'lambda = {0.0:10.1f}, h = {k:.6e}, error = {err:.6e}')\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\naxs[0].plot(ts, vs1)\naxs[0].plot(ts, ue, ls='--')\naxs[0].set_title(r'$\\lambda = 0.0$')\n\n# First with lambda\nlam = -10.0\n\ndef f2(t, u):\n    return lam * (u - np.cos(t)) - np.sin(t)\n\n# Solve\nk  = 1e-3\nnumsteps = np.ceil(te / k).astype(int)\nk = te / numsteps \nts = np.arange(numsteps + 1) * k\nue = np.cos(ts)\n\n# Execute Euler's method\nvs2 = fwd_euler(u0, f2, k, numsteps)\nerr = np.abs(vs2 - ue)[-1]  # Grab last error\n\nprint(f'lambda = {lam:10.1f}, h = {k:.6e}, error = {err:.6e}')\n\naxs[1].plot(ts, vs2)\naxs[1].plot(ts, ue, ls='--')\naxs[1].set_title(r'$\\lambda = -10.0$')\n\n# Let's do it again \nlam = -2100.0\n\ndef f2(t, u):\n    return lam * (u - np.cos(t)) - np.sin(t)\n\n# Solve\nk  = 1e-3\nnumsteps = np.ceil(te / k).astype(int)\nk = te / numsteps \nts = np.arange(numsteps + 1) * k\nue = np.cos(ts)\n\n# Execute Euler's method\nvs3 = fwd_euler(u0, f2, k, numsteps)\nerr = np.abs(vs3 - ue)[-1]  # Grab last error\n\nprint(f'lambda = {lam:10.1f}, h = {k:.6e}, error = {err:.6e}')\n\naxs[2].plot(ts, vs3)\naxs[2].plot(ts, ue, ls='--')\naxs[2].set_title(r'$\\lambda = -2100.0$')"
  },
  {
   "cell_type": "markdown",
   "id": "c4b46242-0d9e-49a5-99c3-671759332b20",
   "metadata": {},
   "source": "## Rescuing the unstable case\n\nFor $\\lambda = -2100$, we reduce $h$ until we satisfy the stability condition $h|\\lambda| \\leq 2$. The output shows the error dropping dramatically once $h$ is small enough."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579443e-5f2d-468b-9211-c79d0c575a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do it again \n",
    "lam = -2100.0\n",
    "\n",
    "def f2(t, u):\n",
    "    return lam * (u - np.cos(t)) - np.sin(t)\n",
    "\n",
    "# Solve\n",
    "for k in [0.000976, 0.000950, 0.0008, 0.0004]:\n",
    "    numsteps = np.ceil(te / k).astype(int)\n",
    "    k = te / numsteps \n",
    "    ts = np.arange(numsteps + 1) * k\n",
    "    ue = np.cos(ts)\n",
    "    \n",
    "    # Execute Euler's method\n",
    "    vs3 = fwd_euler(u0, f2, k, numsteps)\n",
    "    err = np.abs(vs3 - ue)[-1]  # Grab last error\n",
    "    \n",
    "    print(f'lambda = {lam:10.1f}, h = {k:.2e}, error = {err:.6e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1524a56-619d-47e4-8d56-7fef9d8fb9b1",
   "metadata": {},
   "source": "# Stiffness: Forward vs Backward Euler\n\nWe add a stiff term $-100(u - \\cos t)$ to the ODE and compare forward and backward Euler across a range of step sizes. Forward Euler requires $h < 2/100 = 0.02$ for stability; for larger $h$ the error explodes. Backward Euler is unconditionally stable and converges at the expected $O(h)$ rate regardless of step size."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f615c78-060f-4c13-b2e3-1a8c88146d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logkset = np.arange(1, 10)\n",
    "\n",
    "tmax = 2.\n",
    "exact = np.cos(tmax)\n",
    "ex_errs = []\n",
    "\n",
    "for logk in logkset:\n",
    "    k = 0.5**logk\n",
    "    nsteps = int(tmax/k)\n",
    "    t = 0\n",
    "    v = np.cos(t)\n",
    "    f = -np.sin(t)\n",
    "    for i in range(nsteps):\n",
    "        t = i * k\n",
    "        # f = -np.sin(t)\n",
    "        f = -np.sin(t) - 100 * (v - np.cos(t))\n",
    "        vnew = v + k * f\n",
    "        v = vnew\n",
    "    \n",
    "    error = np.abs(v - exact)\n",
    "    ex_errs.append(error)\n",
    "\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Time step $k$')\n",
    "plt.loglog(0.5**logkset, ex_errs, marker='o')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90a978-20e0-4fa2-b32c-1d42dea57c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "logkset = np.arange(1, 10)\n",
    "\n",
    "tmax = 2.\n",
    "exact = np.cos(tmax)\n",
    "errs = []\n",
    "\n",
    "for logk in logkset:\n",
    "    k = 0.5**logk\n",
    "    nsteps = int(tmax/k)\n",
    "    t = 0\n",
    "    v = np.cos(t)\n",
    "    f = -np.sin(t)\n",
    "    for i in range(nsteps):\n",
    "        t = i * k\n",
    "        # First\n",
    "        vnew = v + k * (-np.sin(t + k))\n",
    "        # Second\n",
    "        # vnew = (v + k * (-np.sin(k + t) + 100*np.cos(t + k))) / (1 + k * 100)\n",
    "        v = vnew\n",
    "    \n",
    "    error = np.abs(v - exact)\n",
    "    errs.append(error)\n",
    "\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Time step $k$')\n",
    "plt.loglog(0.5**logkset, errs, marker='o', label='Bwd Euler')\n",
    "plt.loglog(0.5**logkset, ex_errs, marker='o', label='Fwd Euler')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}