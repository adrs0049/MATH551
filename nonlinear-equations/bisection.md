# Bisection Method

:::{tip} Big Idea
The bisection method exploits the Intermediate Value Theorem: if a continuous function changes sign on an interval, it must have a root there. By repeatedly halving the interval, we trap the root with guaranteed convergence.
:::

## The Intermediate Value Theorem

:::{prf:theorem} Intermediate Value Theorem
:label: thm-ivt

Suppose $f \in \mathcal{C}([a, b])$ and $f(a) < \phi < f(b)$. Then there exists $c \in (a, b)$ such that $f(c) = \phi$.
:::

:::{prf:remark} Application to Root Finding
:label: rmk-ivt-roots
:class: dropdown

Setting $\phi = 0$: if $f$ is continuous and $f(a)f(b) < 0$ (i.e., $f$ changes sign), then a root exists in $(a, b)$.

This gives us an **existence guarantee**—something bisection exploits at every step.
:::

## The Algorithm

The bisection method is a **divide and conquer** algorithm.

:::{prf:algorithm} Bisection
:label: alg-bisection

**Input:** $f$, interval $[a, b]$ with $f(a)f(b) < 0$, tolerance $\varepsilon$

**Output:** Approximate root $c$

1. **while** $(b - a)/2 > \varepsilon$:
2. $\qquad c \gets (a + b)/2$
3. $\qquad$ **if** $f(c) = 0$: **return** $c$
4. $\qquad$ **if** $\operatorname{sign}(f(c)) = \operatorname{sign}(f(a))$:
5. $\qquad\qquad a \gets c$
6. $\qquad$ **else**:
7. $\qquad\qquad b \gets c$
8. **return** $(a + b)/2$
:::

At each step, the interval width is **halved**. This geometric reduction is why bisection always converges.

## Convergence Analysis

:::{prf:theorem} Bisection Convergence
:label: thm-bisection-convergence

Suppose $f \in \mathcal{C}([a, b])$ with $f(a)f(b) < 0$. If $\{x_n\}$ is the sequence of midpoints generated by bisection, then

$$
|\ell - x_n| \leq \frac{b - a}{2^{n+1}}
$$

where $\ell$ is the root.
:::

:::{prf:proof}
:class: dropdown

At each step, the interval width halves. The midpoint $x_n$ is at most half the interval width away from any point in the interval, including the root $\ell$:

$$
|\ell - x_n| \leq \frac{1}{2}|b_n - a_n| \leq \frac{1}{2^2}|b_{n-1} - a_{n-1}| \leq \cdots \leq \frac{b - a}{2^{n+1}}
$$

Since $2^{n+1} \to \infty$, we have $x_n \to \ell$.
:::

:::{prf:corollary} Predictable Iteration Count
:label: cor-bisection-iterations

To achieve $|x_n - \ell| < \varepsilon$, we need:

$$
n \geq \left\lceil \log_2\left(\frac{b - a}{2\varepsilon}\right) \right\rceil
$$
:::

:::{prf:proof}
:class: dropdown

We want $\frac{b-a}{2^{n+1}} < \varepsilon$, which gives:

$$
2^{n+1} > \frac{b-a}{\varepsilon} \implies n+1 > \log_2\left(\frac{b-a}{\varepsilon}\right)
$$

Rearranging and taking the ceiling gives the result.
:::

:::{prf:example} How Many Iterations?
:label: ex-bisection-iterations
:class: dropdown

To find $\sqrt{2}$ (root of $x^2 - 2 = 0$) on $[1, 2]$ with accuracy $10^{-10}$:

$$
n \geq \log_2\left(\frac{1}{2 \times 10^{-10}}\right) = \log_2(5 \times 10^9) \approx 32.2
$$

So **33 iterations** suffice—regardless of the function's complexity!
:::

## Demonstrations

:::{seealso}
[Bisection Method Demo](bisection-demo.ipynb) — Finding $\sqrt{3}$, interval visualization, Newton comparison
:::

## Advantages and Disadvantages

**Advantages:**
- **Guaranteed convergence** — Cannot fail if you have a sign change
- **Predictable** — Know exactly how many iterations before you start
- **Simple** — Only needs function evaluations, no derivatives
- **Robust** — Immune to pathologies that break faster methods

**Disadvantages:**
- **Slow** — Linear convergence; ~34 iterations for 10 digits (vs. 4-5 for Newton)
- **Requires bracket** — Must find interval with sign change first
- **Scalar only** — No generalization to systems in $\mathbb{R}^n$

**Practical wisdom:** Use bisection to get close, then switch to Newton for speed.
