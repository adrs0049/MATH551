{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Differentiation\n",
    "\n",
    ":::{tip} Big Idea\n",
    "In value space, differentiation becomes matrix multiplication: $\\mathbf{f}' = D\\mathbf{f}$. The Chebyshev differentiation matrix achieves **spectral accuracy**—exponentially better than finite differences for smooth functions.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import dct, idct\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Idea\n",
    "\n",
    "Given function values $f_0, f_1, \\ldots, f_n$ at Chebyshev points $x_0, x_1, \\ldots, x_n$:\n",
    "\n",
    "1. Construct the unique polynomial $p(x)$ of degree $\\leq n$ interpolating these values\n",
    "2. Differentiate: $p'(x)$ is also a polynomial\n",
    "3. Evaluate $p'(x_i)$ at the same points\n",
    "\n",
    "This defines a **linear map** from values to derivative values:\n",
    "$$\n",
    "p'(x_i) = \\sum_{j=0}^{n} D_{ij} f_j\n",
    "$$\n",
    "\n",
    "The matrix $D$ is the **Chebyshev differentiation matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Derivation from Lagrange Polynomials\n\nThe interpolant in value space is:\n$$\np_n(x) = \\sum_{j=0}^{n} f_j \\ell_j(x)\n$$\n\nwhere $\\ell_j(x)$ are the Lagrange basis polynomials. Taking the derivative:\n$$\np_n'(x) = \\sum_{j=0}^{n} f_j \\ell_j'(x)\n$$\n\nAt the data points $x_i$, we want $f_i' = p_n'(x_i)$. This defines the differentiation matrix:\n$$\nD_{ij} := \\ell_j'(x_i)\n$$\n\n### Deriving the Formula\n\nFrom the [barycentric formula](lagrange.md), recall that:\n$$\n\\ell_j(x) = \\ell(x) \\frac{\\lambda_j}{x - x_j}\n$$\nwhere $\\ell(x) = \\prod_k (x - x_k)$ is the node polynomial and $\\lambda_j = 1/\\ell'(x_j)$ are the barycentric weights.\n\nTaking the derivative (product rule):\n$$\n\\ell_j'(x) = \\ell'(x) \\frac{\\lambda_j}{x - x_j} - \\ell(x) \\frac{\\lambda_j}{(x - x_j)^2}\n$$\n\nEvaluating at $x = x_i$ for $i \\neq j$:\n$$\nD_{ij} = \\ell_j'(x_i) = \\frac{\\lambda_j}{\\lambda_i(x_i - x_j)}\n$$\n\nFor the diagonal entries, we use the fact that the derivative of the constant function 1 is 0:\n$$\n0 = \\frac{d}{dx}\\left(\\sum_j \\ell_j(x)\\right) = \\sum_j \\ell_j'(x)\n$$\n\nSo:\n$$\nD_{ii} = -\\sum_{k \\neq i} D_{ik}\n$$\n\n:::{prf:property} Differentiation Matrix Entries\n:label: prop-diff-matrix-entries-cheb\n\n$$\nD_{ij} = \\begin{cases}\n\\displaystyle\\frac{\\lambda_j/\\lambda_i}{x_i - x_j} & i \\neq j \\\\[2ex]\n\\displaystyle-\\sum_{k \\neq i} D_{ik} & i = j\n\\end{cases}\n$$\n\nFor Chebyshev points, $\\lambda_j = (-1)^j \\delta_j$ where $\\delta_0 = \\delta_n = 1/2$ and $\\delta_j = 1$ otherwise.\n:::",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebpts(n):\n",
    "    \"\"\"n+1 Chebyshev points on [-1, 1].\"\"\"\n",
    "    return np.cos(np.pi * np.arange(n+1) / n)\n",
    "\n",
    "def cheb_diff_matrix(n):\n",
    "    \"\"\"Chebyshev differentiation matrix (n+1) x (n+1).\n",
    "    \n",
    "    Returns D, x where D @ f gives derivative values.\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return np.array([[0.0]]), np.array([1.0])\n",
    "    \n",
    "    # Chebyshev points\n",
    "    x = chebpts(n)\n",
    "    \n",
    "    # Barycentric weights: c_j = (-1)^j * delta_j\n",
    "    c = np.ones(n+1)\n",
    "    c[0] = 2\n",
    "    c[n] = 2\n",
    "    c = c * ((-1) ** np.arange(n+1))\n",
    "    \n",
    "    # Build differentiation matrix using barycentric formula\n",
    "    # D_ij = (c_j / c_i) / (x_i - x_j) for i != j\n",
    "    X = x.reshape(-1, 1) - x.reshape(1, -1)  # x_i - x_j\n",
    "    X[np.diag_indices(n+1)] = 1  # Avoid division by zero\n",
    "    \n",
    "    C = c.reshape(-1, 1) / c.reshape(1, -1)  # c_i / c_j (note: inverted for formula)\n",
    "    \n",
    "    D = C / X\n",
    "    \n",
    "    # Diagonal entries: D_ii = -sum_{j != i} D_ij\n",
    "    D[np.diag_indices(n+1)] = 0\n",
    "    D[np.diag_indices(n+1)] = -D.sum(axis=1)\n",
    "    \n",
    "    return D, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Differentiation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, x = cheb_diff_matrix(16)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Matrix structure\n",
    "im = axes[0].imshow(D, cmap='RdBu_r', aspect='equal')\n",
    "axes[0].set_title('Chebyshev Differentiation Matrix $D$ (n=16)')\n",
    "axes[0].set_xlabel('Column $j$')\n",
    "axes[0].set_ylabel('Row $i$')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Magnitude (log scale)\n",
    "im2 = axes[1].imshow(np.log10(np.abs(D) + 1e-16), cmap='viridis', aspect='equal')\n",
    "axes[1].set_title(r'$\\log_{10}|D_{ij}|$')\n",
    "axes[1].set_xlabel('Column $j$')\n",
    "axes[1].set_ylabel('Row $i$')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- $D$ is **dense** (not sparse like finite difference matrices)\n",
    "- Largest entries are near the corners (endpoints of the interval)\n",
    "- Antisymmetric structure: $D_{ij} = -D_{n-i, n-j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Differentiating $\\sin(\\pi x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16\n",
    "D, x = cheb_diff_matrix(n)\n",
    "\n",
    "# Function and exact derivative\n",
    "f = np.sin(np.pi * x)\n",
    "df_exact = np.pi * np.cos(np.pi * x)\n",
    "\n",
    "# Spectral derivative\n",
    "df_spectral = D @ f\n",
    "\n",
    "# Plot\n",
    "x_fine = np.linspace(-1, 1, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_fine, np.pi * np.cos(np.pi * x_fine), 'b-', label=r\"$f'(x) = \\pi\\cos(\\pi x)$ (exact)\", linewidth=2)\n",
    "plt.plot(x, df_spectral, 'ro', markersize=8, label='Spectral derivative')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$f'(x)$\")\n",
    "plt.title(r\"Spectral Differentiation of $\\sin(\\pi x)$\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum error: {np.max(np.abs(df_spectral - df_exact)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 16 points, we achieve near-machine precision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral vs Finite Differences: Convergence\n",
    "\n",
    "Let's compare spectral differentiation with second-order finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff_2nd(f, x):\n",
    "    \"\"\"Second-order central finite differences.\"\"\"\n",
    "    n = len(x)\n",
    "    h = x[1] - x[0]  # Assumes uniform spacing\n",
    "    df = np.zeros(n)\n",
    "    \n",
    "    # Central differences in interior\n",
    "    df[1:-1] = (f[2:] - f[:-2]) / (2 * h)\n",
    "    \n",
    "    # One-sided at boundaries\n",
    "    df[0] = (-3*f[0] + 4*f[1] - f[2]) / (2*h)\n",
    "    df[-1] = (3*f[-1] - 4*f[-2] + f[-3]) / (2*h)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "f_func = lambda x: np.sin(np.pi * x)\n",
    "df_func = lambda x: np.pi * np.cos(np.pi * x)\n",
    "\n",
    "ns = 2**np.arange(2, 10)\n",
    "err_fd = []\n",
    "err_spectral = []\n",
    "\n",
    "for n in ns:\n",
    "    # Finite differences on uniform grid\n",
    "    x_uni = np.linspace(-1, 1, n+1)\n",
    "    f_uni = f_func(x_uni)\n",
    "    df_fd = finite_diff_2nd(f_uni, x_uni)\n",
    "    err_fd.append(np.max(np.abs(df_fd - df_func(x_uni))))\n",
    "    \n",
    "    # Spectral on Chebyshev grid\n",
    "    D, x = cheb_diff_matrix(n)\n",
    "    f = f_func(x)\n",
    "    df_spectral = D @ f\n",
    "    err_spectral.append(np.max(np.abs(df_spectral - df_func(x))))\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(ns, err_fd, 'o-', label='Finite Differences (2nd order)', linewidth=2, markersize=8)\n",
    "plt.loglog(ns, err_spectral, 's-', label='Spectral', linewidth=2, markersize=8)\n",
    "plt.loglog(ns, 1e-15*np.ones_like(ns), 'k--', label='Machine precision', alpha=0.5)\n",
    "\n",
    "# Reference slope\n",
    "plt.loglog(ns, 5/ns**2, 'r:', label=r'$O(n^{-2})$', linewidth=1.5)\n",
    "\n",
    "plt.xlabel('Number of points $n$')\n",
    "plt.ylabel('Maximum error')\n",
    "plt.title(r'Differentiation Error: $f(x) = \\sin(\\pi x)$')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([1e-16, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Table\n",
    "\n",
    "Let's see the numbers explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"   n     FD Error      Spectral Error    Ratio\")\n",
    "print(\"-\" * 55)\n",
    "for n, e_fd, e_sp in zip(ns, err_fd, err_spectral):\n",
    "    ratio = e_fd / e_sp if e_sp > 1e-16 else float('inf')\n",
    "    print(f\"{n:4d}    {e_fd:10.2e}    {e_sp:14.2e}    {ratio:10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** With $n=32$ points:\n",
    "- Finite differences: ~$10^{-3}$ error\n",
    "- Spectral: ~$10^{-14}$ error\n",
    "\n",
    "That's **11 orders of magnitude** better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Function Smoothness\n",
    "\n",
    "Spectral accuracy depends on function smoothness. Let's compare:\n",
    "\n",
    "1. **Analytic:** $f(x) = e^{\\sin(\\pi x)}$\n",
    "2. **$C^3$:** $f(x) = |x|^5$ (has discontinuous 4th derivative at 0)\n",
    "3. **$C^1$:** $f(x) = |x|^3$ (has discontinuous 2nd derivative at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test functions and their derivatives\n",
    "test_functions = [\n",
    "    {\n",
    "        'f': lambda x: np.exp(np.sin(np.pi * x)),\n",
    "        'df': lambda x: np.pi * np.cos(np.pi * x) * np.exp(np.sin(np.pi * x)),\n",
    "        'name': r'$e^{\\sin(\\pi x)}$ (analytic)',\n",
    "        'smooth': 'analytic'\n",
    "    },\n",
    "    {\n",
    "        'f': lambda x: np.abs(x)**5,\n",
    "        'df': lambda x: 5 * np.abs(x)**4 * np.sign(x),\n",
    "        'name': r'$|x|^5$ ($C^4$)',\n",
    "        'smooth': 'C4'\n",
    "    },\n",
    "    {\n",
    "        'f': lambda x: np.abs(x)**3,\n",
    "        'df': lambda x: 3 * np.abs(x)**2 * np.sign(x),\n",
    "        'name': r'$|x|^3$ ($C^2$)',\n",
    "        'smooth': 'C2'\n",
    "    },\n",
    "]\n",
    "\n",
    "ns = 2**np.arange(2, 10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, test in zip(axes, test_functions):\n",
    "    err_sp = []\n",
    "    for n in ns:\n",
    "        D, x = cheb_diff_matrix(n)\n",
    "        f = test['f'](x)\n",
    "        df = D @ f\n",
    "        df_exact = test['df'](x)\n",
    "        err_sp.append(np.max(np.abs(df - df_exact)))\n",
    "    \n",
    "    ax.loglog(ns, err_sp, 's-', linewidth=2, markersize=8)\n",
    "    ax.loglog(ns, 1e-15*np.ones_like(ns), 'k--', alpha=0.5)\n",
    "    ax.set_xlabel('$n$')\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_title(test['name'])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([1e-16, 10])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Why Spectral Accuracy?\n\nThe error in spectral differentiation equals the error in polynomial approximation of $f'$:\n$$\n\\|f' - (Df)_{\\text{interp}}\\|_\\infty \\leq C \\cdot \\|f' - p'_n\\|_\\infty\n$$\n\nFor analytic $f$, polynomial approximation converges exponentially, so differentiation does too.\n\n**Contrast with finite differences:**\n- FD approximates derivatives **locally** via Taylor expansion\n- Spectral uses **global** polynomial information\n- Global information → exponential convergence for smooth functions\n\nThis is the fundamental difference: finite differences \"see\" only nearby points, while spectral methods use information from the entire domain.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. Analytic function: exponential convergence to machine precision\n",
    "2. $C^4$ function: algebraic convergence $O(n^{-4})$\n",
    "3. $C^2$ function: algebraic convergence $O(n^{-2})$\n",
    "\n",
    "The convergence rate matches the smoothness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Derivatives\n",
    "\n",
    "For the second derivative, simply square the matrix: $D^{(2)} = D^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "D, x = cheb_diff_matrix(n)\n",
    "D2 = D @ D\n",
    "\n",
    "# Test: d²/dx² of sin(πx) = -π² sin(πx)\n",
    "f = np.sin(np.pi * x)\n",
    "d2f_exact = -np.pi**2 * np.sin(np.pi * x)\n",
    "d2f_spectral = D2 @ f\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "x_fine = np.linspace(-1, 1, 200)\n",
    "plt.plot(x_fine, -np.pi**2 * np.sin(np.pi * x_fine), 'b-', linewidth=2, label='Exact')\n",
    "plt.plot(x, d2f_spectral, 'ro', markersize=8, label='Spectral')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r\"$f''(x)$\")\n",
    "plt.title(r\"Second Derivative of $\\sin(\\pi x)$\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(x, np.abs(d2f_spectral - d2f_exact), 'ko-')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('Absolute error')\n",
    "plt.title('Error in second derivative')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum error in second derivative: {np.max(np.abs(d2f_spectral - d2f_exact)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues of the Differentiation Matrix\n",
    "\n",
    "The eigenvalues of $D$ determine stability for time-dependent problems."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Stability Considerations\n\n**Numerical stability:** The matrix $D$ can be ill-conditioned for large $n$. When possible, use the explicit barycentric formula rather than forming $D$ explicitly.\n\n**For time-dependent PDEs:** The large eigenvalues of $D$ create **stiffness**. For a problem like $u_t = u_x$:\n- Explicit time-stepping (forward Euler) requires $\\Delta t = O(n^{-2})$\n- Implicit methods or exponential integrators are preferred\n\n## Properties of the Differentiation Matrix\n\n1. **Dense:** $D$ is an $(n+1) \\times (n+1)$ full matrix—no sparsity to exploit\n2. **Antisymmetric structure:** $D_{ij} = -D_{n-i,n-j}$\n3. **Large entries near boundaries:** Spectral methods concentrate resolution at endpoints\n4. **Eigenvalues:** Complex, spread widely (source of stiffness)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, n in zip(axes, [8, 16, 32]):\n",
    "    D, x = cheb_diff_matrix(n)\n",
    "    eigs = np.linalg.eigvals(D)\n",
    "    \n",
    "    ax.plot(eigs.real, eigs.imag, 'o', markersize=5)\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "    ax.set_xlabel(r'Re$(\\lambda)$')\n",
    "    ax.set_ylabel(r'Im$(\\lambda)$')\n",
    "    ax.set_title(f'Eigenvalues of $D$ (n={n})')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The eigenvalues spread widely—this creates **stiffness** for time-stepping. Explicit methods require tiny time steps; implicit methods are preferred."
   ]
  },
  {
   "cell_type": "code",
   "source": "def cheb_diff_coeffs(c):\n    \"\"\"Differentiate in coefficient space. O(n) complexity.\"\"\"\n    n = len(c) - 1\n    if n == 0:\n        return np.array([0.0])\n    \n    dc = np.zeros(n)\n    dc[n-1] = 2 * n * c[n]\n    if n >= 2:\n        dc[n-2] = 2 * (n-1) * c[n-1]\n    \n    for k in range(n-3, -1, -1):\n        dc[k] = dc[k+2] + 2 * (k+1) * c[k+1]\n    \n    dc[0] /= 2  # Adjust for c_0 convention\n    return dc\n\n# Compare with matrix method\nfrom scipy.fft import dct\n\ndef vals2coeffs(values):\n    n = len(values) - 1\n    if n == 0:\n        return values.copy()\n    coeffs = dct(values[::-1], type=1) / n\n    coeffs[0] /= 2\n    coeffs[-1] /= 2\n    return coeffs\n\ndef coeffs2vals(coeffs):\n    n = len(coeffs) - 1\n    if n == 0:\n        return coeffs.copy()\n    coeffs_scaled = coeffs.copy()\n    coeffs_scaled[0] *= 2\n    coeffs_scaled[-1] *= 2\n    return dct(coeffs_scaled, type=1)[::-1] / 2\n\n# Test: derivative of sin(πx)\nn = 20\nx = chebpts(n)\nf = np.sin(np.pi * x)\nc = vals2coeffs(f)\ndc = cheb_diff_coeffs(c)\ndf_from_coeffs = coeffs2vals(np.append(dc, 0))  # Pad to same length\n\nD, _ = cheb_diff_matrix(n)\ndf_from_matrix = D @ f\n\nprint(\"Comparison of differentiation methods:\")\nprint(f\"  Matrix method max error:      {np.max(np.abs(df_from_matrix - np.pi*np.cos(np.pi*x))):.2e}\")\nprint(f\"  Coefficient method max error: {np.max(np.abs(df_from_coeffs - np.pi*np.cos(np.pi*x))):.2e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Connection to Coefficient Space\n\nDifferentiation can also be done in **coefficient space** via a recurrence relation. If $f(x) = \\sum_{k=0}^n c_k T_k(x)$, then $f'(x) = \\sum_{k=0}^{n-1} c'_k T_k(x)$ where:\n\n$$\nc'_{n-1} = 2n c_n, \\quad c'_{n-2} = 2(n-1) c_{n-1}\n$$\n$$\nc'_k = c'_{k+2} + 2(k+1) c_{k+1} \\quad \\text{for } k = n-3, \\ldots, 0\n$$\n\nThis is $O(n)$ compared to $O(n^2)$ for matrix multiplication. Modern spectral codes often work in coefficient space for this reason.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Solving a BVP\n",
    "\n",
    "Solve $u'' = e^{4x}$ on $[-1, 1]$ with $u(-1) = u(1) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32\n",
    "D, x = cheb_diff_matrix(n)\n",
    "D2 = D @ D\n",
    "\n",
    "# Right-hand side\n",
    "f = np.exp(4 * x)\n",
    "\n",
    "# Apply boundary conditions: u(-1) = u(1) = 0\n",
    "# Remove first and last rows/columns, solve interior system\n",
    "D2_int = D2[1:-1, 1:-1]\n",
    "f_int = f[1:-1]\n",
    "\n",
    "# Solve\n",
    "u_int = np.linalg.solve(D2_int, f_int)\n",
    "u = np.zeros(n+1)\n",
    "u[1:-1] = u_int\n",
    "\n",
    "# Exact solution\n",
    "u_exact = (np.exp(4*x) - np.sinh(4)*x - np.cosh(4)) / 16\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, u_exact, 'b-', linewidth=2, label='Exact')\n",
    "plt.plot(x, u, 'ro', markersize=6, label='Spectral')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$u(x)$')\n",
    "plt.title(r\"Solution of $u'' = e^{4x}$\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(x, np.abs(u - u_exact) + 1e-16, 'ko-')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum error: {np.max(np.abs(u - u_exact)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Method | Convergence | Matrix Type | Best For |\n",
    "|--------|-------------|-------------|----------|\n",
    "| 2nd-order FD | $O(h^2)$ | Sparse | Large problems, limited smoothness |\n",
    "| 4th-order FD | $O(h^4)$ | Sparse | Moderate accuracy |\n",
    "| Spectral | $O(e^{-\\alpha n})$ | Dense | Smooth problems, high accuracy |\n",
    "\n",
    "**Key insight:** For smooth functions, spectral methods achieve 14 digits of accuracy where finite differences achieve 4. The cost of dense matrix operations is repaid by needing far fewer points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}