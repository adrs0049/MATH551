{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Fast Inverse Square Root\n",
    "\n",
    "This notebook explores the famous fast inverse square root algorithm from Quake III Arena, demonstrating how understanding IEEE 754 floating-point representation enables creative numerical tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ieee-header",
   "metadata": {},
   "source": [
    "## IEEE 754 Floating-Point Representation\n",
    "\n",
    "A 32-bit float consists of:\n",
    "- 1 sign bit\n",
    "- 8 exponent bits (biased by 127)\n",
    "- 23 mantissa bits\n",
    "\n",
    "The value represented is:\n",
    "$$x = (-1)^s \\cdot 2^{E-127} \\cdot (1 + m)$$\n",
    "\n",
    "where $m = M/2^{23}$ is the mantissa fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "float-bits",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_bits(f):\n",
    "    \"\"\"Convert a float to its 32-bit integer representation.\"\"\"\n",
    "    return struct.unpack('I', struct.pack('f', f))[0]\n",
    "\n",
    "def bits_to_float(i):\n",
    "    \"\"\"Convert a 32-bit integer to its float representation.\"\"\"\n",
    "    return struct.unpack('f', struct.pack('I', i))[0]\n",
    "\n",
    "def show_float_bits(f):\n",
    "    \"\"\"Display the bit representation of a float.\"\"\"\n",
    "    bits = float_to_bits(f)\n",
    "    binary = f'{bits:032b}'\n",
    "    sign = binary[0]\n",
    "    exponent = binary[1:9]\n",
    "    mantissa = binary[9:]\n",
    "    \n",
    "    E = int(exponent, 2)\n",
    "    M = int(mantissa, 2)\n",
    "    \n",
    "    print(f\"Float: {f}\")\n",
    "    print(f\"Bits:  {sign} | {exponent} | {mantissa}\")\n",
    "    print(f\"       S   E={E} (bias 127)   M={M}\")\n",
    "    print(f\"       = (-1)^{sign} × 2^{E-127} × (1 + {M}/2^23)\")\n",
    "    return bits\n",
    "\n",
    "# Example\n",
    "print(\"=\" * 60)\n",
    "show_float_bits(2.0)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "show_float_bits(0.15625)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "log-approx-header",
   "metadata": {},
   "source": [
    "## The Key Insight: Bit Pattern ≈ Logarithm\n",
    "\n",
    "Taking $\\log_2$ of a positive float:\n",
    "$$\\log_2(x) = (E - 127) + \\log_2(1 + m)$$\n",
    "\n",
    "Since $m \\in [0, 1)$, we can approximate $\\log_2(1 + m) \\approx m + \\sigma$ where $\\sigma \\approx 0.0430$.\n",
    "\n",
    "Substituting $m = M/2^{23}$:\n",
    "$$\\log_2(x) \\approx \\frac{1}{2^{23}}(M + 2^{23} E) + \\sigma - 127 = \\frac{\\text{Int}(x)}{2^{23}} + \\sigma - 127$$\n",
    "\n",
    "**The integer interpretation of float bits approximates the logarithm!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log-approx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the logarithm approximation\n",
    "x_vals = np.logspace(-2, 2, 100).astype(np.float32)\n",
    "\n",
    "true_log = np.log2(x_vals)\n",
    "\n",
    "# Integer interpretation scaled\n",
    "sigma = 0.0430\n",
    "approx_log = np.array([float_to_bits(x) / 2**23 + sigma - 127 for x in x_vals])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(x_vals, true_log, 'b-', linewidth=2, label=r'$\\log_2(x)$')\n",
    "ax1.plot(x_vals, approx_log, 'r--', linewidth=2, label='Bit approximation')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel(r'$\\log_2(x)$')\n",
    "ax1.set_title('Logarithm from Bit Pattern')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_vals, np.abs(true_log - approx_log), 'g-', linewidth=2)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('Absolute error')\n",
    "ax2.set_title('Approximation Error')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum error: {np.max(np.abs(true_log - approx_log)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algorithm-header",
   "metadata": {},
   "source": [
    "## The Fast Inverse Square Root Algorithm\n",
    "\n",
    "We want $y = 1/\\sqrt{x}$. Taking logarithms:\n",
    "$$\\log_2(y) = -\\frac{1}{2}\\log_2(x)$$\n",
    "\n",
    "Using the bit-pattern approximation:\n",
    "$$\\text{Int}(y) \\approx \\frac{3}{2} \\cdot 2^{23}(127 - \\sigma) - \\frac{1}{2}\\text{Int}(x)$$\n",
    "\n",
    "The magic number `0x5f3759df` ≈ 1597463007 comes from this formula!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_inverse_sqrt(x):\n",
    "    \"\"\"\n",
    "    The famous fast inverse square root from Quake III.\n",
    "    Returns an approximation to 1/sqrt(x).\n",
    "    \"\"\"\n",
    "    # Convert to 32-bit float\n",
    "    x = np.float32(x)\n",
    "    \n",
    "    # Get integer representation\n",
    "    i = float_to_bits(x)\n",
    "    \n",
    "    # The magic: bit manipulation gives initial guess\n",
    "    i = 0x5f3759df - (i >> 1)  # \"what the fuck?\"\n",
    "    \n",
    "    # Convert back to float\n",
    "    y = bits_to_float(i)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def fast_inverse_sqrt_newton(x, iterations=1):\n",
    "    \"\"\"\n",
    "    Fast inverse square root with Newton refinement.\n",
    "    \"\"\"\n",
    "    x = np.float32(x)\n",
    "    x2 = x * 0.5\n",
    "    \n",
    "    # Initial guess from bit manipulation\n",
    "    i = float_to_bits(x)\n",
    "    i = 0x5f3759df - (i >> 1)\n",
    "    y = bits_to_float(i)\n",
    "    \n",
    "    # Newton iterations: y = y * (1.5 - x/2 * y^2)\n",
    "    for _ in range(iterations):\n",
    "        y = y * (1.5 - x2 * y * y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Test on a few values\n",
    "test_values = [0.25, 1.0, 2.0, 4.0, 10.0, 100.0]\n",
    "\n",
    "print(f\"{'x':>8} | {'True 1/√x':>12} | {'Bit trick':>12} | {'+ Newton':>12} | {'Bit err':>10} | {'Newton err':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for x in test_values:\n",
    "    true_val = 1.0 / np.sqrt(x)\n",
    "    bit_approx = fast_inverse_sqrt(x)\n",
    "    newton_approx = fast_inverse_sqrt_newton(x, iterations=1)\n",
    "    \n",
    "    bit_err = abs(bit_approx - true_val) / true_val\n",
    "    newton_err = abs(newton_approx - true_val) / true_val\n",
    "    \n",
    "    print(f\"{x:8.2f} | {true_val:12.8f} | {bit_approx:12.8f} | {newton_approx:12.8f} | {bit_err:10.2e} | {newton_err:10.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "newton-header",
   "metadata": {},
   "source": [
    "## Newton's Method Refinement\n",
    "\n",
    "The bit manipulation gives ~3.4% relative error. Newton's method refines this.\n",
    "\n",
    "To find $y = 1/\\sqrt{x}$, we solve $g(y) = 1/y^2 - x = 0$.\n",
    "\n",
    "Newton's iteration:\n",
    "$$y_{n+1} = y_n - \\frac{g(y_n)}{g'(y_n)} = y_n - \\frac{1/y_n^2 - x}{-2/y_n^3} = y_n\\left(\\frac{3}{2} - \\frac{x}{2}y_n^2\\right)$$\n",
    "\n",
    "This is exactly: `y = y * (1.5 - x/2 * y * y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "newton-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Newton's method convergence\n",
    "x = 2.0\n",
    "true_val = 1.0 / np.sqrt(x)\n",
    "\n",
    "print(f\"Computing 1/√{x} = {true_val:.15f}\\n\")\n",
    "\n",
    "# Initial guess from bit trick\n",
    "i = float_to_bits(np.float32(x))\n",
    "i = 0x5f3759df - (i >> 1)\n",
    "y = bits_to_float(i)\n",
    "\n",
    "print(f\"{'Iteration':>10} | {'Approximation':>18} | {'Relative Error':>15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Bit trick':>10} | {y:18.15f} | {abs(y - true_val)/true_val:15.2e}\")\n",
    "\n",
    "x2 = x * 0.5\n",
    "for n in range(1, 5):\n",
    "    y = y * (1.5 - x2 * y * y)\n",
    "    rel_err = abs(y - true_val) / true_val\n",
    "    print(f\"{n:>10} | {y:18.15f} | {rel_err:15.2e}\")\n",
    "    if rel_err < 1e-15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-analysis-header",
   "metadata": {},
   "source": [
    "## Error Analysis Over Range\n",
    "\n",
    "Let's see how the algorithm performs across a wide range of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.logspace(-4, 4, 1000).astype(np.float32)\n",
    "true_vals = 1.0 / np.sqrt(x_vals)\n",
    "\n",
    "# Bit trick only\n",
    "bit_approx = np.array([fast_inverse_sqrt(x) for x in x_vals])\n",
    "bit_rel_err = np.abs(bit_approx - true_vals) / true_vals\n",
    "\n",
    "# With 1 Newton iteration\n",
    "newton1_approx = np.array([fast_inverse_sqrt_newton(x, 1) for x in x_vals])\n",
    "newton1_rel_err = np.abs(newton1_approx - true_vals) / true_vals\n",
    "\n",
    "# With 2 Newton iterations\n",
    "newton2_approx = np.array([fast_inverse_sqrt_newton(x, 2) for x in x_vals])\n",
    "newton2_rel_err = np.abs(newton2_approx - true_vals) / true_vals\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.semilogy(x_vals, bit_rel_err, 'b-', alpha=0.7, label='Bit trick only')\n",
    "ax.semilogy(x_vals, newton1_rel_err, 'r-', alpha=0.7, label='+ 1 Newton iteration')\n",
    "ax.semilogy(x_vals, newton2_rel_err, 'g-', alpha=0.7, label='+ 2 Newton iterations')\n",
    "\n",
    "ax.axhline(y=np.finfo(np.float32).eps, color='gray', linestyle='--', \n",
    "           label=f'Single precision ε ≈ {np.finfo(np.float32).eps:.1e}')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('Relative Error', fontsize=12)\n",
    "ax.set_title('Fast Inverse Square Root: Error Analysis', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.set_ylim(1e-10, 1e-1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Bit trick max error:     {np.max(bit_rel_err):.2%}\")\n",
    "print(f\"+ 1 Newton max error:    {np.max(newton1_rel_err):.4%}\")\n",
    "print(f\"+ 2 Newton max error:    {np.max(newton2_rel_err):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magic-header",
   "metadata": {},
   "source": [
    "## The Magic Number\n",
    "\n",
    "The constant `0x5f3759df` can be derived from the formula:\n",
    "$$\\text{Magic} = \\frac{3}{2} \\cdot 2^{23}(127 - \\sigma)$$\n",
    "\n",
    "where $\\sigma$ is a correction constant. Different values of $\\sigma$ give different magic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magic-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_magic_number(sigma):\n",
    "    \"\"\"Compute the magic number for a given sigma.\"\"\"\n",
    "    return int(1.5 * 2**23 * (127 - sigma))\n",
    "\n",
    "# The original magic number\n",
    "original_magic = 0x5f3759df\n",
    "\n",
    "# Derive sigma from the original magic number\n",
    "sigma_derived = 127 - original_magic / (1.5 * 2**23)\n",
    "\n",
    "print(f\"Original magic number: 0x{original_magic:08x} = {original_magic}\")\n",
    "print(f\"Derived σ: {sigma_derived:.6f}\")\n",
    "print()\n",
    "\n",
    "# Try different sigma values\n",
    "print(f\"{'σ':>10} | {'Magic (hex)':>14} | {'Magic (dec)':>12}\")\n",
    "print(\"-\" * 42)\n",
    "for sigma in [0.0, 0.0430, 0.0450, sigma_derived]:\n",
    "    magic = compute_magic_number(sigma)\n",
    "    print(f\"{sigma:10.4f} | 0x{magic:08x} | {magic:12d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Comparison: What Made This Fast?\n",
    "\n",
    "The algorithm's speed came from avoiding expensive operations:\n",
    "\n",
    "| Operation | Fast Inverse Sqrt | Standard Method |\n",
    "|-----------|-------------------|------------------|\n",
    "| Division | 0 | 1 |\n",
    "| Square root | 0 | 1 |\n",
    "| Multiplication | 3 | 1+ |\n",
    "| Subtraction | 2 | 0 |\n",
    "| Bit shift | 1 | 0 |\n",
    "\n",
    "In the 1990s, division and square root were ~10-40x slower than multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simple timing comparison (for demonstration, not rigorous benchmarking)\n",
    "n_trials = 100000\n",
    "x_test = np.random.uniform(0.1, 100, n_trials).astype(np.float32)\n",
    "\n",
    "# Standard method\n",
    "start = time.perf_counter()\n",
    "for x in x_test:\n",
    "    result = 1.0 / np.sqrt(x)\n",
    "standard_time = time.perf_counter() - start\n",
    "\n",
    "# Fast inverse sqrt (with 1 Newton iteration)\n",
    "start = time.perf_counter()\n",
    "for x in x_test:\n",
    "    result = fast_inverse_sqrt_newton(x, 1)\n",
    "fast_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Timing comparison ({n_trials:,} evaluations):\")\n",
    "print(f\"  Standard (1/√x):        {standard_time*1000:.2f} ms\")\n",
    "print(f\"  Fast inverse sqrt:      {fast_time*1000:.2f} ms\")\n",
    "print(f\"\\nNote: Modern CPUs have dedicated rsqrt instructions,\")\n",
    "print(f\"so Python overhead dominates. In 1999, this was ~4x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The fast inverse square root demonstrates several key ideas:\n",
    "\n",
    "1. **Floating-point representation matters**: Understanding IEEE 754 enables creative algorithms\n",
    "\n",
    "2. **Bit patterns encode information**: The integer interpretation of float bits approximates the logarithm\n",
    "\n",
    "3. **Initial guess + Newton refinement**: A clever initial guess plus one Newton iteration achieves good accuracy\n",
    "\n",
    "4. **Trade-offs in numerical computing**: Sometimes approximate-but-fast beats exact-but-slow\n",
    "\n",
    "5. **Historical context matters**: This hack was essential for real-time 3D graphics; modern CPUs have dedicated instructions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
